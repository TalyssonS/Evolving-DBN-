{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ddcb530",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:04.454141Z",
     "start_time": "2022-11-28T01:15:04.438491Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils as DM\n",
    "from datetime import datetime, timedelta\n",
    "FullOpt =  True\n",
    "from pgmpy.inference import VariableElimination\n",
    "import jupyter_contrib_nbextensions\n",
    "import random\n",
    "import warnings\n",
    "import sys \n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import psycopg2 as pg\n",
    "import sqlalchemy as sq\n",
    "import networkx as nx\n",
    "logging.disable()\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "FullOpt =  True #True é para usar HC e false busca exaustiva\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import BDeuScore\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b788a428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:05.825083Z",
     "start_time": "2022-11-28T01:15:05.820064Z"
    }
   },
   "outputs": [],
   "source": [
    "def smooth(y, box_pts):\n",
    "    vi = y[0]\n",
    "    vf = y[len(y)-1]\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    y_smooth[0] = vi\n",
    "    y_smooth[len(y_smooth)-1] = vf\n",
    "    return y_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59d9a041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:06.172025Z",
     "start_time": "2022-11-28T01:15:06.116095Z"
    }
   },
   "outputs": [],
   "source": [
    "def errorf (real,forecast):\n",
    "    error=[]\n",
    "    for i in range(len(real)):\n",
    "        error.append(real[i]-forecast[i])\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bfb8cf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:06.526816Z",
     "start_time": "2022-11-28T01:15:06.520833Z"
    }
   },
   "outputs": [],
   "source": [
    "def open_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE\n",
    "    '''\n",
    "    conn = pg.connect(dbname='postgres', user = 'postgres', password = 123, host = 'localhost')\n",
    "    return conn\n",
    "def get_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE AND RETURN THE SQLACHEMY ENGINE OBJECT\n",
    "    -----------\n",
    "    output: object\n",
    "        SQLACHEMY ENGINE OBJECT - POSTGRESQL DATABASE CONNECTION\n",
    "    '''\n",
    "    user = 'postgres'\n",
    "    password = 123\n",
    "    host = 'localhost'\n",
    "    port = 5432\n",
    "    database = 'postgres'\n",
    "    return sq.create_engine(url=\"postgresql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4bdb801",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:06.952970Z",
     "start_time": "2022-11-28T01:15:06.945954Z"
    }
   },
   "outputs": [],
   "source": [
    "def blanket(model,variable):\n",
    "    '''\n",
    "    Function to extract the Markov Blanket of the target variable (reduces the structure)\n",
    "    -----------\n",
    "    input:\n",
    "    model: list\n",
    "        list of edges\n",
    "        \n",
    "    variable: str\n",
    "        name of the target variable\n",
    "        \n",
    "    output:\n",
    "    blanket: list\n",
    "        list of edges \n",
    "    '''\n",
    "    blanket=[]\n",
    "    sons=[]\n",
    "    for i in model:\n",
    "        if i[0]==variable or i[1]==variable:\n",
    "            blanket.append(i)\n",
    "        if i[0]==variable:\n",
    "            sons.append(i[1])\n",
    "    for i in model:\n",
    "        if i[1] in sons and i[0]!=variable:\n",
    "            blanket.append(i)\n",
    "    return blanket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a8ca24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:07.425981Z",
     "start_time": "2022-11-28T01:15:07.413982Z"
    }
   },
   "outputs": [],
   "source": [
    "def verifica_remove_ciclos(edges):\n",
    "    '''\n",
    "    Function to verify if the edges is a DAG and to try remove cycles\n",
    "    -----------\n",
    "    input:\n",
    "    edges: list\n",
    "        list of edges\n",
    "                \n",
    "    output:\n",
    "    blanket: list\n",
    "        list of edges \n",
    "    '''\n",
    "    edgesdag = edges #recebe o próprio modelo\n",
    "    #Verifica se tem ciclos e tenta remover invertendo uma aresta\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:  # (3) flip single edge\n",
    "            edges2 = edgesdag.copy()\n",
    "            edges2.extend([i[::-1]])\n",
    "            new_edges = edges2.copy()\n",
    "            new_edges.remove(i)\n",
    "            if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                edgesdag = new_edges.copy()\n",
    "                break\n",
    "    #Verifica se tem ciclos e tenta remover invertendo duas arestas\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:\n",
    "            for j in edgesdag:# (3) flip two edges\n",
    "                if i != j:\n",
    "                    edges2 = edgesdag.copy()\n",
    "                    edges2.extend([i[::-1]])\n",
    "                    edges2.extend([j[::-1]])\n",
    "                    new_edges = edges2.copy()\n",
    "                    new_edges.remove(i)\n",
    "                    new_edges.remove(j)\n",
    "                    if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                        edgesdag = new_edges.copy()\n",
    "                        breaker = True\n",
    "                        break\n",
    "            if breaker:\n",
    "                break\n",
    "    #Verifica se tem ciclos e tenta remover invertendo uma aresta e excluindo uma aresta\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:\n",
    "            for j in edgesdag:# (3) flip two edges\n",
    "                if i != j:\n",
    "                    edges2 = edgesdag.copy()\n",
    "                    edges2.extend([i[::-1]])\n",
    "                    new_edges = edges2.copy()\n",
    "                    new_edges.remove(i)\n",
    "                    new_edges.remove(j)\n",
    "                    if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                        edgesdag = new_edges.copy()\n",
    "                        breaker = True\n",
    "                        break\n",
    "            if breaker:\n",
    "                break\n",
    "    return edgesdag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1e2199",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:08.023585Z",
     "start_time": "2022-11-28T01:15:08.017565Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_dates(pais):\n",
    "    q = '''select distinct cast(\"Date\" as DATE) as datas from pre_processed_data.dbn_features_selected_{pais} order by datas'''.format(pais=pais)\n",
    "    conn = open_connection()\n",
    "    date = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    datas = date['datas'].tolist()\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d4c1a84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:08.458134Z",
     "start_time": "2022-11-28T01:15:08.454128Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_features_selected_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "916e726f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:09.063173Z",
     "start_time": "2022-11-28T01:15:09.056138Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset_allfeatures(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b9c761a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:09.687623Z",
     "start_time": "2022-11-28T01:15:09.681635Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_edges_frequencies(best_model, edges_possibilities, edges_frequency):\n",
    "    if not edges_possibilities:\n",
    "        edges_possibilities = best_model\n",
    "        for p in range(len(edges_possibilities)):\n",
    "            edges_frequency.append(1)\n",
    "    else:\n",
    "        for v in range(len(best_model)):\n",
    "            if best_model[v] not in edges_possibilities:\n",
    "                edges_possibilities.append(best_model[v])\n",
    "                edges_frequency.append(1)\n",
    "            else:\n",
    "                for f in range(len(edges_possibilities)):\n",
    "                    if best_model[v] == edges_possibilities[f]:\n",
    "                        edges_frequency[f]=edges_frequency[f]+1\n",
    "    return edges_possibilities, edges_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fb9e2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:10.496857Z",
     "start_time": "2022-11-28T01:15:10.480875Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_threshold_select_edges(k, edges_possibilities, edges_frequency):\n",
    "    fth = 1/3+np.sqrt(2/k)\n",
    "    if fth>0.4:\n",
    "        fth=0.4\n",
    "    edges_frequency_v=[edges_frequency[i]/k for i in range(len(edges_frequency))]\n",
    "    edges=[]\n",
    "    for i in range(len(edges_possibilities)):\n",
    "        if edges_frequency_v[i]>=fth and edges_possibilities[i] not in edges:\n",
    "            if edges_possibilities[i][::-1] not in edges_possibilities:\n",
    "                edges.append(edges_possibilities[i])\n",
    "            else: \n",
    "                if edges_frequency_v[i] > edges_frequency_v[edges_possibilities.index(edges_possibilities[i][::-1])]:\n",
    "                    edges.append(edges_possibilities[i])\n",
    "                else: \n",
    "                    edges.append(edges_possibilities[i][::-1])\n",
    "        elif edges_frequency_v[i]<fth:\n",
    "            for j in range(len(edges_possibilities)):\n",
    "                if edges_possibilities[i]==edges_possibilities[j][::-1]:\n",
    "                    if edges_frequency_v[i]+edges_frequency_v[j]>=fth:\n",
    "                        if edges_frequency_v[i]>edges_frequency_v[j]:\n",
    "                            edges.append(edges_possibilities[i])\n",
    "                        if edges_frequency_v[i]<edges_frequency_v[j]:\n",
    "                            edges.append(edges_possibilities[j])\n",
    "                        if edges_frequency_v[i]==edges_frequency_v[j]:\n",
    "                            auxci=0\n",
    "                            auxcj=0\n",
    "                            for s in range(len(edges)):\n",
    "                                if edges[s]==edges_possibilities[i]:\n",
    "                                    auxci=auxci+1\n",
    "                                if edges[s]==edges_possibilities[j]:\n",
    "                                    auxcj=auxcj+1\n",
    "                            if auxci>0:\n",
    "                                edges.append(edges_possibilities[i])\n",
    "                            elif auxcj>0:\n",
    "                                edges.append(edges_possibilities[j])\n",
    "                            else: \n",
    "                                import random\n",
    "                                edges.append(random.choice([edges_possibilities[i],edges_possibilities[j]]))\n",
    "    edges = list(set(edges))\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81c9d6f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:11.214666Z",
     "start_time": "2022-11-28T01:15:11.199012Z"
    }
   },
   "outputs": [],
   "source": [
    "def bins_values(pais):\n",
    "    q = '''select \"Emission\" from pre_processed_data.bins_{pais} where \"Emission\" is not null'''.format(pais = pais)\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0149c6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:11.631577Z",
     "start_time": "2022-11-28T01:15:11.619608Z"
    }
   },
   "outputs": [],
   "source": [
    "def real_values(pais, data):\n",
    "    q = '''select \"Emission\" from pre_processed_data.{pais} where \"Date\" = '{dataf}' '''.format(pais = pais, dataf = (data+timedelta(days = 1)).strftime(\"%Y/%m/%d\"))\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb5f78fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:15:13.479812Z",
     "start_time": "2022-11-28T01:15:13.453884Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(pais):\n",
    "    #initialize auxiliary variables\n",
    "    k=1 #total days used\n",
    "    target_variable = 'Emission'\n",
    "    edges_possibilities = []\n",
    "    edges_frequency = []\n",
    "    timemodel = []\n",
    "    timeinference = []\n",
    "    forecast_values = pd.DataFrame()\n",
    "\n",
    "    #read all available dates\n",
    "    dates = get_all_dates(pais)\n",
    "\n",
    "    #begin the forecast experiment\n",
    "    for i in tqdm(dates[7:8]):\n",
    "        #dataset to save de forecast values\n",
    "        forecast_aux = pd.DataFrame()\n",
    "        forecast_date = []\n",
    "        forecast_hour = []\n",
    "        forecast_v = []\n",
    "        #dataset to learn the model\n",
    "        data_learn = get_dataset(pais,i, i+timedelta(days = 0))\n",
    "        #structural learning with the dataset of day i\n",
    "        data_learn.drop(['Date','Hour'], axis = 1, inplace = True)\n",
    "        ti = time.time()\n",
    "        best_model = DM.EdgesModel(data_learn, FullOpt)[0]\n",
    "\n",
    "        #get the markov blanket\n",
    "        best_model = blanket(best_model, target_variable)\n",
    "\n",
    "        #update the edges frequencies\n",
    "        edges_possibilities, edges_frequency = update_edges_frequencies(best_model, edges_possibilities, edges_frequency)\n",
    "\n",
    "        #update threshold and select the edges\n",
    "        edges = update_threshold_select_edges(k, edges_possibilities, edges_frequency)\n",
    "\n",
    "        tf = time.time()\n",
    "        timemodel.append(tf-ti)\n",
    "        #forecast initial in day 8 (fit from 01 until 07)\n",
    "        if i >= dates[6] and i+timedelta(days = 1) in dates:\n",
    "            #bins\n",
    "            bins = bins_values(pais)\n",
    "            \n",
    "            #fit dataset (last 7 days)\n",
    "            fit_data = get_dataset(pais,i-timedelta(days = 6), i)\n",
    "            fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 6), i)\n",
    "\n",
    "            #predict data of the entire day\n",
    "            predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "            predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "\n",
    "            #detects independent variables\n",
    "            independentes=[]\n",
    "            for col in fit_data.columns:\n",
    "                if col not in list(map(lambda x: x[0],edges))+list(map(lambda x: x[1],edges)):\n",
    "                    independentes.append(col)\n",
    "\n",
    "            #drop independent columns\n",
    "            fit_data.drop(independentes, axis=1, inplace = True)\n",
    "            predict_data_day.drop(independentes, axis=1, inplace = True)\n",
    "\n",
    "            #transform data in levels (limitation of PGMPY)\n",
    "            levels = {}\n",
    "            aux = fit_dataall.copy()\n",
    "            aux = aux.append(predict_dataall)\n",
    "            for var in aux.columns:\n",
    "                levels[var] = set(aux[var])\n",
    "                fit_dataall[var] = fit_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                predict_dataall[var] = predict_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                if var in fit_data.columns:\n",
    "                    fit_data[var] = fit_data[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                    predict_data_day[var] = predict_data_day[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            predict_data_day = predict_data_day.astype(int)\n",
    "            \n",
    "            #Using the edges, get the bayesian model object\n",
    "            model = BayesianModel(edges)\n",
    "\n",
    "            #aux\n",
    "            aux_fore = []\n",
    "        \n",
    "            #predict each point of day i+1\n",
    "            ti_inf = time.time()\n",
    "            for h in range(len(predict_data_day)):\n",
    "                forecast_date.append(i+timedelta(days = 1))\n",
    "                forecast_hour.append(h)\n",
    "                predict_data = predict_data_day.iloc[[h]]\n",
    "                predictall = predict_dataall.iloc[[h]]\n",
    "                fit_datah = fit_data.loc[0:len(fit_data)-3+h] #tau = 3 (forecast horizon)\n",
    "                \n",
    "                #fit the bayesian model to get de CPTs\n",
    "                model.fit(fit_datah)\n",
    "                model.get_cpds(node = target_variable)\n",
    "\n",
    "                #drop all variable in time window T+1 (unknown values - future states)\n",
    "                for c in predict_data.columns:\n",
    "                    if '-1' not in c:\n",
    "                        predict_data[c] = predictall[c+str('-1')]\n",
    "                del predict_data[target_variable]\n",
    "                \n",
    "                #solve limitation of unknown level \n",
    "                for col in predict_data.columns:\n",
    "                    predict_data[col][predict_data[col]>=len(set(fit_datah[col]))] = len(set(fit_datah[col]))-1\n",
    "                y_pred = model.predict(predict_data)\n",
    "                y_pred[target_variable] = y_pred[target_variable].replace(np.arange(0,len(levels[target_variable])),levels[target_variable])\n",
    "                for v in y_pred[target_variable]:\n",
    "                    aux_fore.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "                fit_data = fit_data.append(predict_data_day.loc[h-3:h-3]).reset_index(drop = True) \n",
    "            forecast_aux['Date'] = forecast_date\n",
    "            forecast_aux['Hour'] = forecast_hour\n",
    "            forecast_aux['Emissions Forecast'] = smooth(aux_fore,5)\n",
    "            real_value = real_values(pais, i)\n",
    "            forecast_aux[target_variable] = real_value[target_variable]\n",
    "            forecast_values = forecast_values.append(forecast_aux)\n",
    "            tf_inf = time.time()\n",
    "            timeinference.append(tf_inf-ti_inf)\n",
    "        k = k+1\n",
    "    #save the results on postgres\n",
    "    df_edges = pd.DataFrame()\n",
    "    df_edges['edges'] = edges_possibilities\n",
    "    df_edges['frequencia'] = edges_frequency\n",
    "    df_edges['total days'] = k-1\n",
    "    df_time_model = pd.DataFrame()\n",
    "    df_time_model['tempo'] = timemodel\n",
    "    df_time_inference = pd.DataFrame()\n",
    "    df_time_inference['tempo'] = timeinference\n",
    "    \n",
    "    df_edges.to_sql(name='edges_frequency_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    df_time_model.to_sql(name='time_model_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    df_time_inference.to_sql(name='time_inference_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    forecast_values.to_sql(name='forecast_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef32431b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:26:11.527432Z",
     "start_time": "2022-11-28T01:26:08.290129Z"
    }
   },
   "outputs": [],
   "source": [
    "#read all available dates\n",
    "pais = 'espanha'\n",
    "dates = get_all_dates(pais)\n",
    "i = dates[0]\n",
    "data_learn = get_dataset(pais,i, i+timedelta(days = 0))\n",
    "#structural learning with the dataset of day i\n",
    "data_learn.drop(['Date','Hour'], axis = 1, inplace = True)\n",
    "best_model = DM.EdgesModel(data_learn, FullOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4daa60e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:26:12.387282Z",
     "start_time": "2022-11-28T01:26:12.378288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Emission', 'Fossil Gas'),\n",
       " ('Emission', 'W. Onshore'),\n",
       " ('Emission-1', 'W. Onshore-1'),\n",
       " ('Emission-1', 'Fossil Gas-1'),\n",
       " ('Emission-1', 'Hard coal-1'),\n",
       " ('Fossil Gas', 'Hydro P. S.'),\n",
       " ('Fossil Gas-1', 'Biomass'),\n",
       " ('Fossil Gas-1', 'W. Onshore'),\n",
       " ('Hard coal', 'Hydro R. P.'),\n",
       " ('Hard coal', 'W. Onshore'),\n",
       " ('W. Onshore', 'Fossil Gas'),\n",
       " ('W. Onshore-1', 'Fossil Gas-1'),\n",
       " ('W. Onshore-1', 'Hydro R. P.-1'),\n",
       " ('W. Onshore-1', 'W. Onshore'),\n",
       " ('Biomass', 'Emission'),\n",
       " ('Biomass', 'Hard coal'),\n",
       " ('Biomass', 'Hydro P. S.-1'),\n",
       " ('Biomass', 'W. Onshore'),\n",
       " ('Biomass-1', 'Emission-1')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c0f86af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T23:45:04.172772Z",
     "start_time": "2022-11-27T23:44:43.705553Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'show_progress'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 106\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m predict_data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m    105\u001b[0m     predict_data[col][predict_data[col]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(fit_datah[col]))] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(fit_datah[col]))\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 106\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m y_pred[target_variable] \u001b[38;5;241m=\u001b[39m y_pred[target_variable]\u001b[38;5;241m.\u001b[39mreplace(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(levels[target_variable])),levels[target_variable])\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m y_pred[target_variable]:\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'show_progress'"
     ]
    }
   ],
   "source": [
    "pais = 'espanha'\n",
    "#initialize auxiliary variables\n",
    "target_variable = 'Emission'\n",
    "edges_possibilities = []\n",
    "edges_frequency = []\n",
    "timemodel = []\n",
    "timeinference = []\n",
    "forecast_values = pd.DataFrame()\n",
    "pais = 'espanha'\n",
    "\n",
    "#read all available dates\n",
    "dates = get_all_dates(pais)\n",
    "\n",
    "#begin the forecast experiment\n",
    "i = dates[len(dates)-2]\n",
    "#dataset to save de forecast values\n",
    "forecast_aux = pd.DataFrame()\n",
    "forecast_date = []\n",
    "forecast_hour = []\n",
    "forecast_v = []\n",
    "\n",
    "q = '''select * \n",
    "from results.edges_frequency_espanha '''\n",
    "conn = open_connection()\n",
    "dados = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "k = max(dados['total days'])\n",
    "edges_possibilities = dados['edges']\n",
    "edges_frequency = dados['frequencia']\n",
    "edges = update_threshold_select_edges(k, edges_possibilities, edges_frequency)\n",
    "if len(edges)==0:\n",
    "    edges.append(('Emission-1','Emission')) \n",
    "edges = blanket(best_model, target_variable)\n",
    "#forecast initial in day 8 (fit from 01 until 07)\n",
    "if i >= dates[6] and i+timedelta(days = 1) in dates:\n",
    "    #bins\n",
    "    bins = bins_values(pais)\n",
    "\n",
    "    #fit dataset (last 3 days)\n",
    "    fit_data = get_dataset(pais,i-timedelta(days = 2), i)\n",
    "    fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 2), i)\n",
    "\n",
    "    #predict data of the entire day\n",
    "    predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "    predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "\n",
    "    #detects independent variables\n",
    "    independentes=[]\n",
    "    for col in fit_data.columns:\n",
    "        if col not in list(map(lambda x: x[0],edges))+list(map(lambda x: x[1],edges)):\n",
    "            independentes.append(col)\n",
    "\n",
    "    #drop independent columns\n",
    "    fit_data.drop(independentes, axis=1, inplace = True)\n",
    "    predict_data_day.drop(independentes, axis=1, inplace = True)\n",
    "\n",
    "    #transform data in levels (limitation of PGMPY)\n",
    "    levels = {}\n",
    "    aux = fit_dataall.copy()\n",
    "    aux = aux.append(predict_dataall)\n",
    "    for var in aux.columns:\n",
    "        levels[var] = set(aux[var])\n",
    "        fit_dataall[var] = fit_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "        predict_dataall[var] = predict_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "        if var in fit_data.columns:\n",
    "            fit_data[var] = fit_data[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            predict_data_day[var] = predict_data_day[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "    predict_data_day = predict_data_day.astype(int)\n",
    "\n",
    "    #Using the edges, get the bayesian model object\n",
    "    model = BayesianModel(edges)\n",
    "\n",
    "    #aux\n",
    "    aux_fore = []\n",
    "\n",
    "    #predict each point of day i+1\n",
    "    ti_inf = time.time()\n",
    "    for h in range(len(predict_data_day)):\n",
    "        forecast_date.append(i+timedelta(days = 1))\n",
    "        forecast_hour.append(h)\n",
    "        predict_data = predict_data_day.iloc[[h]]\n",
    "        predictall = predict_dataall.iloc[[h]]\n",
    "        fit_datah = fit_data.loc[0:len(fit_data)-3+h] #tau = 3 (forecast horizon)\n",
    "\n",
    "        smote = RandomOverSampler(random_state = 42)\n",
    "        y = fit_datah[target_variable]\n",
    "        X = fit_datah.copy()\n",
    "        del X[target_variable]\n",
    "        Xc, yc = smote.fit_resample(X,y)\n",
    "        fit_datah = Xc\n",
    "        fit_datah[target_variable] = yc\n",
    "\n",
    "        #fit the bayesian model to get de CPTs\n",
    "        model.fit(fit_datah)\n",
    "        model.get_cpds(node = target_variable)\n",
    "\n",
    "        #drop all variable in time window T+1 (unknown values - future states)\n",
    "        for c in predict_data.columns:\n",
    "            if '-1' not in c:\n",
    "                predict_data[c] = predictall[c+str('-1')]\n",
    "        del predict_data[target_variable]\n",
    "\n",
    "        #solve limitation of unknown level \n",
    "        for col in predict_data.columns:\n",
    "            predict_data[col][predict_data[col]>=len(set(fit_datah[col]))] = len(set(fit_datah[col]))-1\n",
    "        y_pred = model.predict(predict_data, n_jobs = 1)\n",
    "        y_pred[target_variable] = y_pred[target_variable].replace(np.arange(0,len(levels[target_variable])),levels[target_variable])\n",
    "        for v in y_pred[target_variable]:\n",
    "            aux_fore.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "        fit_data = fit_data.append(predict_data_day.loc[h-3:h-3]).reset_index(drop = True) \n",
    "    forecast_aux['Date'] = forecast_date\n",
    "    forecast_aux['Hour'] = forecast_hour\n",
    "    forecast_aux['Emissions Forecast'] = smooth(aux_fore,5)\n",
    "    real_value = real_values(pais, i)\n",
    "    forecast_aux[target_variable] = real_value[target_variable]\n",
    "    forecast_values = forecast_values.append(forecast_aux)\n",
    "    tf_inf = time.time()\n",
    "    timeinference.append(tf_inf-ti_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b00e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "597e8003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T23:54:41.947619Z",
     "start_time": "2022-11-27T23:54:41.878805Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'show_progress'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m predict_data \u001b[38;5;241m=\u001b[39m predict_data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     11\u001b[0m predict_data\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'show_progress'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "values = pd.DataFrame(np.random.randint(low=0, high=2, size=(1000, 5)),\n",
    "                      columns=['A', 'B', 'C', 'D', 'E'])\n",
    "train_data = values[:800]\n",
    "predict_data = values[800:]\n",
    "model = [('A', 'B'), ('C', 'B'), ('C', 'D'), ('B', 'E')]\n",
    "model = BayesianModel(model)\n",
    "model.fit(train_data)\n",
    "predict_data = predict_data.copy()\n",
    "predict_data.drop('E', axis=1, inplace=True)\n",
    "y_pred = model.predict(predict_data,show_progress = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "332b6ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T21:50:08.770087Z",
     "start_time": "2022-11-27T21:50:08.759115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      0\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "195    1\n",
       "196    1\n",
       "197    1\n",
       "198    1\n",
       "199    1\n",
       "Name: E, Length: 200, dtype: int32"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred['E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38520ec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T21:30:25.132782Z",
     "start_time": "2022-11-27T21:30:25.122810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f410fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T03:08:34.595683Z",
     "start_time": "2022-11-27T03:08:34.588702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2021, 12, 30)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31c2e197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T03:08:59.654371Z",
     "start_time": "2022-11-27T03:08:59.388710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0072808523830155"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_Belgicaln\" fa '''\n",
    "conn = open_connection()\n",
    "datasetln = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "median_absolute_error(datasetln['Emission'][datasetln['Date']==forecast_date[0]],datasetln['Emissions Forecast'][datasetln['Date']==forecast_date[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a169a6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T03:27:19.544865Z",
     "start_time": "2022-11-27T03:27:19.535890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.047452111757096"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc4cc31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T04:03:35.098018Z",
     "start_time": "2022-09-13T04:01:16.762742Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [02:16<00:00, 136.23s/it]\n"
     ]
    }
   ],
   "source": [
    "main('espanha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85e8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb35543c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-12T02:47:09.170586Z",
     "start_time": "2022-09-12T02:47:09.162610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSE:  6.31499349207514\n",
      "MAE:  37.81498384126137\n",
      "MedAE:  34.9104457095642\n"
     ]
    }
   ],
   "source": [
    "print(\"NRMSE: \",np.sqrt((mean_squared_error(aux_fore['Emission'],aux_fore['Emissions Forecast']))/(max(aux_fore['Emission'])-min(aux_fore['Emission']))))\n",
    "print(\"MAE: \", mean_absolute_error(aux_fore['Emission'],aux_fore['Emissions Forecast']))\n",
    "print(\"MedAE: \", median_absolute_error(aux_fore['Emission'],aux_fore['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "62498754",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-12T03:20:20.531580Z",
     "start_time": "2022-09-12T03:20:20.525596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSE:  5.735187661252892\n",
      "MAE:  36.10798746002474\n",
      "MedAE:  32.001057093187285\n"
     ]
    }
   ],
   "source": [
    "print(\"NRMSE: \",np.sqrt((mean_squared_error(aux_fore['Emission'],smooth(aux_fore['Emissions Forecast'],5)))/(max(aux_fore['Emission'])-min(aux_fore['Emission']))))\n",
    "print(\"MAE: \", mean_absolute_error(aux_fore['Emission'],smooth(aux_fore['Emissions Forecast'],5)))\n",
    "print(\"MedAE: \", median_absolute_error(aux_fore['Emission'],smooth(aux_fore['Emissions Forecast'],5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c32fcef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-12T12:03:14.103098Z",
     "start_time": "2022-09-12T12:03:14.094121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSE:  7.051595561925361\n",
      "MAE:  44.12230251179301\n",
      "MedAE:  46.690401681751496\n"
     ]
    }
   ],
   "source": [
    "print(\"NRMSE: \",np.sqrt((mean_squared_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))/(max(forecast_values['Emission'])-min(forecast_values['Emission']))))\n",
    "print(\"MAE: \", mean_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))\n",
    "print(\"MedAE: \", median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "573d5779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-12T12:03:40.367281Z",
     "start_time": "2022-09-12T12:03:40.360299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSE:  6.757625056591128\n",
      "MAE:  44.93697828374403\n",
      "MedAE:  37.69036249889736\n"
     ]
    }
   ],
   "source": [
    "print(\"NRMSE: \",np.sqrt((mean_squared_error(forecast_values['Emission'],smooth(forecast_values['Emissions Forecast'],5)))/(max(forecast_values['Emission'])-min(forecast_values['Emission']))))\n",
    "print(\"MAE: \", mean_absolute_error(forecast_values['Emission'],smooth(forecast_values['Emissions Forecast'],5)))\n",
    "print(\"MedAE: \", median_absolute_error(forecast_values['Emission'],smooth(forecast_values['Emissions Forecast'],5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83117cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a6d2cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:45:26.020410Z",
     "start_time": "2022-11-28T01:44:20.550002Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils as DM\n",
    "from datetime import datetime, timedelta\n",
    "FullOpt =  True\n",
    "from pgmpy.inference import VariableElimination\n",
    "import jupyter_contrib_nbextensions\n",
    "import random\n",
    "import warnings\n",
    "import sys \n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import psycopg2 as pg\n",
    "import sqlalchemy as sq\n",
    "import networkx as nx\n",
    "logging.disable()\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "FullOpt =  True #True é para usar HC e false busca exaustiva\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import BDeuScore\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import gc\n",
    "\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    vi = y[0]\n",
    "    vf = y[len(y)-1]\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    y_smooth[0] = vi\n",
    "    y_smooth[len(y_smooth)-1] = vf\n",
    "    return y_smooth\n",
    "\n",
    "def errorf (real,forecast):\n",
    "    error=[]\n",
    "    for i in range(len(real)):\n",
    "        error.append(real[i]-forecast[i])\n",
    "    return error\n",
    "\n",
    "def open_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE\n",
    "    '''\n",
    "    conn = pg.connect(dbname='postgres', user = 'postgres', password = 123, host = 'localhost')\n",
    "    return conn\n",
    "def get_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE AND RETURN THE SQLACHEMY ENGINE OBJECT\n",
    "    -----------\n",
    "    output: object\n",
    "        SQLACHEMY ENGINE OBJECT - POSTGRESQL DATABASE CONNECTION\n",
    "    '''\n",
    "    user = 'postgres'\n",
    "    password = 123\n",
    "    host = 'localhost'\n",
    "    port = 5432\n",
    "    database = 'postgres'\n",
    "    return sq.create_engine(url=\"postgresql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database))\n",
    "\n",
    "def blanket(model,variable):\n",
    "    '''\n",
    "    Function to extract the Markov Blanket of the target variable (reduces the structure)\n",
    "    -----------\n",
    "    input:\n",
    "    model: list\n",
    "        list of edges\n",
    "        \n",
    "    variable: str\n",
    "        name of the target variable\n",
    "        \n",
    "    output:\n",
    "    blanket: list\n",
    "        list of edges \n",
    "    '''\n",
    "    blanket=[]\n",
    "    sons=[]\n",
    "    for i in model:\n",
    "        if i[0]==variable or i[1]==variable:\n",
    "            blanket.append(i)\n",
    "        if i[0]==variable:\n",
    "            sons.append(i[1])\n",
    "    for i in model:\n",
    "        if i[1] in sons and i[0]!=variable:\n",
    "            blanket.append(i)\n",
    "    return blanket\n",
    "\n",
    "def verifica_remove_ciclos(edges):\n",
    "    '''\n",
    "    Function to verify if the edges is a DAG and to try remove cycles\n",
    "    -----------\n",
    "    input:\n",
    "    edges: list\n",
    "        list of edges\n",
    "                \n",
    "    output:\n",
    "    blanket: list\n",
    "        list of edges \n",
    "    '''\n",
    "    edgesdag = edges #recebe o próprio modelo\n",
    "    #Verifica se tem ciclos e tenta remover invertendo uma aresta\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:  # (3) flip single edge\n",
    "            edges2 = edgesdag.copy()\n",
    "            edges2.extend([i[::-1]])\n",
    "            new_edges = edges2.copy()\n",
    "            new_edges.remove(i)\n",
    "            if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                edgesdag = new_edges.copy()\n",
    "                break\n",
    "    #Verifica se tem ciclos e tenta remover invertendo duas arestas\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:\n",
    "            for j in edgesdag:# (3) flip two edges\n",
    "                if i != j:\n",
    "                    edges2 = edgesdag.copy()\n",
    "                    edges2.extend([i[::-1]])\n",
    "                    edges2.extend([j[::-1]])\n",
    "                    new_edges = edges2.copy()\n",
    "                    new_edges.remove(i)\n",
    "                    new_edges.remove(j)\n",
    "                    if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                        edgesdag = new_edges.copy()\n",
    "                        breaker = True\n",
    "                        break\n",
    "            if breaker:\n",
    "                break\n",
    "    #Verifica se tem ciclos e tenta remover invertendo uma aresta e excluindo uma aresta\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:\n",
    "            for j in edgesdag:# (3) flip two edges\n",
    "                if i != j:\n",
    "                    edges2 = edgesdag.copy()\n",
    "                    edges2.extend([i[::-1]])\n",
    "                    new_edges = edges2.copy()\n",
    "                    new_edges.remove(i)\n",
    "                    new_edges.remove(j)\n",
    "                    if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                        edgesdag = new_edges.copy()\n",
    "                        breaker = True\n",
    "                        break\n",
    "            if breaker:\n",
    "                break\n",
    "    return edgesdag\n",
    "def get_all_dates(pais):\n",
    "    q = '''select distinct cast(\"Date\" as DATE) as datas from pre_processed_data.dbn_features_selected_{pais} order by datas'''.format(pais=pais)\n",
    "    conn = open_connection()\n",
    "    date = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    datas = date['datas'].tolist()\n",
    "    return datas\n",
    "\n",
    "def get_dataset(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_features_selected_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "def get_dataset_allfeatures(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "def update_edges_frequencies(best_model, edges_possibilities, edges_frequency):\n",
    "    if not edges_possibilities:\n",
    "        edges_possibilities = best_model\n",
    "        for p in range(len(edges_possibilities)):\n",
    "            edges_frequency.append(1)\n",
    "    else:\n",
    "        for v in range(len(best_model)):\n",
    "            if best_model[v] not in edges_possibilities:\n",
    "                edges_possibilities.append(best_model[v])\n",
    "                edges_frequency.append(1)\n",
    "            else:\n",
    "                for f in range(len(edges_possibilities)):\n",
    "                    if best_model[v] == edges_possibilities[f]:\n",
    "                        edges_frequency[f]=edges_frequency[f]+1\n",
    "    return edges_possibilities, edges_frequency\n",
    "\n",
    "def update_threshold_select_edges(k, edges_possibilities, edges_frequency):\n",
    "    fth = 1/3+np.sqrt(2/k)\n",
    "    if fth>0.4:\n",
    "        fth=0.4\n",
    "    edges_frequency_v=[edges_frequency[i]/k for i in range(len(edges_frequency))]\n",
    "    edges=[]\n",
    "    for i in range(len(edges_possibilities)):\n",
    "        if edges_frequency_v[i]>=fth and edges_possibilities[i] not in edges:\n",
    "            if edges_possibilities[i][::-1] not in edges_possibilities:\n",
    "                edges.append(edges_possibilities[i])\n",
    "            else: \n",
    "                if edges_frequency_v[i] > edges_frequency_v[edges_possibilities.index(edges_possibilities[i][::-1])]:\n",
    "                    edges.append(edges_possibilities[i])\n",
    "                else: \n",
    "                    edges.append(edges_possibilities[i][::-1])\n",
    "        elif edges_frequency_v[i]<fth:\n",
    "            for j in range(len(edges_possibilities)):\n",
    "                if edges_possibilities[i]==edges_possibilities[j][::-1]:\n",
    "                    if edges_frequency_v[i]+edges_frequency_v[j]>=fth:\n",
    "                        if edges_frequency_v[i]>edges_frequency_v[j]:\n",
    "                            edges.append(edges_possibilities[i])\n",
    "                        if edges_frequency_v[i]<edges_frequency_v[j]:\n",
    "                            edges.append(edges_possibilities[j])\n",
    "                        if edges_frequency_v[i]==edges_frequency_v[j]:\n",
    "                            auxci=0\n",
    "                            auxcj=0\n",
    "                            for s in range(len(edges)):\n",
    "                                if edges[s]==edges_possibilities[i]:\n",
    "                                    auxci=auxci+1\n",
    "                                if edges[s]==edges_possibilities[j]:\n",
    "                                    auxcj=auxcj+1\n",
    "                            if auxci>0:\n",
    "                                edges.append(edges_possibilities[i])\n",
    "                            elif auxcj>0:\n",
    "                                edges.append(edges_possibilities[j])\n",
    "                            else: \n",
    "                                import random\n",
    "                                edges.append(random.choice([edges_possibilities[i],edges_possibilities[j]]))\n",
    "    edges = list(set(edges))\n",
    "    return edges\n",
    "\n",
    "def bins_values(pais):\n",
    "    q = '''select \"Emission\" from pre_processed_data.bins_{pais} where \"Emission\" is not null'''.format(pais = pais)\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def real_values(pais, data):\n",
    "    q = '''select \"Emission\" from pre_processed_data.{pais} where \"Date\" = '{dataf}' '''.format(pais = pais, dataf = (data+timedelta(days = 1)).strftime(\"%Y/%m/%d\"))\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def main(pais):\n",
    "    #initialize auxiliary variables\n",
    "    k=1 #total days used\n",
    "    target_variable = 'Emission'\n",
    "    edges_possibilities = []\n",
    "    edges_frequency = []\n",
    "    timemodel = []\n",
    "    timeinference = []\n",
    "    forecast_values = pd.DataFrame()\n",
    "\n",
    "    #read all available dates\n",
    "    dates = get_all_dates(pais)\n",
    "\n",
    "    #begin the forecast experiment\n",
    "    for i in tqdm(dates):\n",
    "        #dataset to save de forecast values\n",
    "        forecast_aux = pd.DataFrame()\n",
    "        forecast_date = []\n",
    "        forecast_hour = []\n",
    "        forecast_v = []\n",
    "        #dataset to learn the model\n",
    "        data_learn = get_dataset(pais,i, i+timedelta(days = 0))\n",
    "        #structural learning with the dataset of day i\n",
    "        data_learn.drop(['Date','Hour'], axis = 1, inplace = True)\n",
    "        ti = time.time()\n",
    "        best_model = DM.EdgesModel(data_learn, FullOpt)\n",
    "\n",
    "        #get the markov blanket\n",
    "        best_model = blanket(best_model, target_variable)\n",
    "\n",
    "        #update the edges frequencies\n",
    "        edges_possibilities, edges_frequency = update_edges_frequencies(best_model, edges_possibilities, edges_frequency)\n",
    "\n",
    "        #update threshold and select the edges\n",
    "        edges = update_threshold_select_edges(k, edges_possibilities, edges_frequency)\n",
    "        if len(edges)==0:\n",
    "            edges.append(('Emission-1','Emission')) \n",
    "\n",
    "        tf = time.time()\n",
    "        timemodel.append(tf-ti)\n",
    "        #forecast initial in day 8 (fit from 01 until 07)\n",
    "        if i >= dates[6] and i+timedelta(days = 1) in dates:\n",
    "            #bins\n",
    "            bins = bins_values(pais)\n",
    "            \n",
    "            #fit dataset (last 3 days)\n",
    "            fit_data = get_dataset(pais,i-timedelta(days = 2), i)\n",
    "            fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 2), i)\n",
    "\n",
    "            #predict data of the entire day\n",
    "            predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "            predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "\n",
    "            #detects independent variables\n",
    "            independentes=[]\n",
    "            for col in fit_data.columns:\n",
    "                if col not in list(map(lambda x: x[0],edges))+list(map(lambda x: x[1],edges)):\n",
    "                    independentes.append(col)\n",
    "\n",
    "            #drop independent columns\n",
    "            fit_data.drop(independentes, axis=1, inplace = True)\n",
    "            predict_data_day.drop(independentes, axis=1, inplace = True)\n",
    "\n",
    "            #transform data in levels (limitation of PGMPY)\n",
    "            levels = {}\n",
    "            aux = fit_dataall.copy()\n",
    "            aux = aux.append(predict_dataall)\n",
    "            for var in aux.columns:\n",
    "                levels[var] = set(aux[var])\n",
    "                fit_dataall[var] = fit_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                predict_dataall[var] = predict_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                if var in fit_data.columns:\n",
    "                    fit_data[var] = fit_data[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                    predict_data_day[var] = predict_data_day[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            predict_data_day = predict_data_day.astype(int)\n",
    "            \n",
    "            #Using the edges, get the bayesian model object\n",
    "            model = BayesianModel(edges)\n",
    "\n",
    "            #aux\n",
    "            aux_fore = []\n",
    "        \n",
    "            #predict each point of day i+1\n",
    "            ti_inf = time.time()\n",
    "            for h in range(len(predict_data_day)):\n",
    "                forecast_date.append(i+timedelta(days = 1))\n",
    "                forecast_hour.append(h)\n",
    "                predict_data = predict_data_day.iloc[[h]]\n",
    "                predictall = predict_dataall.iloc[[h]]\n",
    "                fit_datah = fit_data.loc[0:len(fit_data)-3+h] #tau = 3 (forecast horizon)\n",
    "                \n",
    "                smote = RandomOverSampler(random_state = 42)\n",
    "                y = fit_datah[target_variable]\n",
    "                X = fit_datah.copy()\n",
    "                del X[target_variable]\n",
    "                Xc, yc = smote.fit_resample(X,y)\n",
    "                fit_datah = Xc\n",
    "                fit_datah[target_variable] = yc\n",
    "                \n",
    "                #fit the bayesian model to get de CPTs\n",
    "                model.fit(fit_datah)\n",
    "                model.get_cpds(node = target_variable)\n",
    "\n",
    "                #drop all variable in time window T+1 (unknown values - future states)\n",
    "                for c in predict_data.columns:\n",
    "                    if '-1' not in c:\n",
    "                        predict_data[c] = predictall[c+str('-1')]\n",
    "                del predict_data[target_variable]\n",
    "                \n",
    "                #solve limitation of unknown level \n",
    "                for col in predict_data.columns:\n",
    "                    predict_data[col][predict_data[col]>=len(set(fit_datah[col]))] = len(set(fit_datah[col]))-1\n",
    "                y_pred = model.predict(predict_data, n_jobs = 1)\n",
    "                y_pred[target_variable] = y_pred[target_variable].replace(np.arange(0,len(levels[target_variable])),levels[target_variable])\n",
    "                for v in y_pred[target_variable]:\n",
    "                    aux_fore.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "                fit_data = fit_data.append(predict_data_day.loc[h-3:h-3]).reset_index(drop = True) \n",
    "            forecast_aux['Date'] = forecast_date\n",
    "            forecast_aux['Hour'] = forecast_hour\n",
    "            forecast_aux['Emissions Forecast'] = smooth(aux_fore,5)\n",
    "            real_value = real_values(pais, i)\n",
    "            forecast_aux[target_variable] = real_value[target_variable]\n",
    "            forecast_values = forecast_values.append(forecast_aux)\n",
    "            tf_inf = time.time()\n",
    "            timeinference.append(tf_inf-ti_inf)\n",
    "        k = k+1\n",
    "    #save the results on postgres\n",
    "    df_edges = pd.DataFrame()\n",
    "    df_edges['edges'] = edges_possibilities\n",
    "    df_edges['frequencia'] = edges_frequency\n",
    "    df_edges['total days'] = k-1\n",
    "    df_time_model = pd.DataFrame()\n",
    "    df_time_model['tempo'] = timemodel\n",
    "    df_time_inference = pd.DataFrame()\n",
    "    df_time_inference['tempo'] = timeinference\n",
    "    \n",
    "    df_edges.to_sql(name='edges_frequency_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    df_time_model.to_sql(name='time_model_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    df_time_inference.to_sql(name='time_inference_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    forecast_values.to_sql(name='forecast_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b16cb5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T01:48:06.680210Z",
     "start_time": "2022-11-28T01:45:43.098820Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████                                                                        | 1/7 [00:02<00:17,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Emission', 'Lignite'), ('Emission', 'W. Offshore'), ('Emission', 'W. Onshore'), ('Emission-1', 'Lignite-1'), ('Emission-1', 'Hard coal'), ('Emission-1', 'Fossil Gas-1'), ('Emission-1', 'W. Offshore-1'), ('Lignite', 'Hard coal'), ('Lignite', 'Fossil Gas'), ('Lignite', 'Hard coal-1'), ('Lignite-1', 'Fossil Gas'), ('Lignite-1', 'Emission'), ('Lignite-1', 'W. Offshore-1'), ('Lignite-1', 'Hard coal-1'), ('Lignite-1', 'Nuclear-1'), ('Lignite-1', 'Lignite'), ('Lignite-1', 'Hard coal'), ('Hard coal', 'Hard coal-1'), ('Hard coal', 'Nuclear'), ('Hard coal-1', 'W. Onshore'), ('W. Onshore-1', 'Emission-1'), ('Fossil Gas-1', 'Lignite-1'), ('Solar', 'W. Onshore'), ('Solar', 'Emission-1'), ('Solar', 'W. Offshore')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████████████████                                                            | 2/7 [00:05<00:13,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Emission', 'Fossil Gas-1'), ('Emission', 'W. Offshore'), ('Emission-1', 'W. Offshore-1'), ('Lignite', 'Emission'), ('Lignite', 'Fossil Gas'), ('Lignite-1', 'Emission-1'), ('Lignite-1', 'W. Onshore-1'), ('Hard coal-1', 'Lignite-1'), ('Hard coal-1', 'Emission-1'), ('W. Onshore', 'Lignite'), ('Fossil Gas', 'Nuclear'), ('Fossil Gas', 'Hard coal'), ('Fossil Gas-1', 'Nuclear-1'), ('Fossil Gas-1', 'Hard coal-1'), ('Fossil Gas-1', 'Hard coal'), ('Fossil Gas-1', 'W. Onshore-1'), ('Nuclear', 'W. Offshore-1'), ('Solar', 'Hard coal'), ('W. Offshore-1', 'Solar')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████████                                                | 3/7 [00:09<00:13,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Emission', 'Lignite'), ('Emission', 'W. Onshore'), ('Emission-1', 'W. Offshore-1'), ('Emission-1', 'W. Onshore-1'), ('Emission-1', 'Solar'), ('Lignite', 'W. Offshore-1'), ('Lignite-1', 'Emission-1'), ('Lignite-1', 'W. Offshore-1'), ('Hard coal', 'Fossil Gas'), ('Hard coal-1', 'Fossil Gas-1'), ('W. Onshore', 'Hard coal'), ('Fossil Gas', 'W. Offshore-1'), ('W. Onshore-1', 'W. Offshore'), ('W. Onshore-1', 'Hard coal-1'), ('Fossil Gas-1', 'Emission'), ('Nuclear-1', 'Fossil Gas'), ('Nuclear-1', 'W. Offshore-1'), ('Nuclear-1', 'W. Onshore-1'), ('Nuclear-1', 'Hard coal-1'), ('Nuclear-1', 'Fossil Gas-1'), ('Solar', 'W. Offshore'), ('Solar', 'W. Onshore'), ('Solar', 'Lignite'), ('W. Offshore', 'Hard coal-1')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████████████████████████████████████                                                | 3/7 [02:22<03:10, 47.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10224\\4196821235.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mdata_learn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Hour'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEdgesModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_learn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFullOpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\dados_alemanha\\Trabalho final\\utils.py\u001b[0m in \u001b[0;36mEdgesModel\u001b[1;34m(data, FullOpt)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mhc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHillClimbSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring_method\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mbdeu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m#val = hc.estimate(initial_model,extra_tabu_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#Exhaustive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\dados_alemanha\\Trabalho final\\utils.py\u001b[0m in \u001b[0;36mestimate\u001b[1;34m(self, start, extra_tabu_list, tabu_length, max_indegree)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;31m#print(iterr)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;31m#print(current_model.edges())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_delta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legal_operations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtabu_list\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mextra_tabu_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_indegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mscore_delta\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score_delta\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\dados_alemanha\\Trabalho final\\utils.py\u001b[0m in \u001b[0;36m_legal_operations\u001b[1;34m(self, model, tabu_list, max_indegree)\u001b[0m\n\u001b[0;32m     47\u001b[0m                     \u001b[0mnew_parents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_parents\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mmax_indegree\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_parents\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mmax_indegree\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                         \u001b[0mscore_delta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_parents\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlocal_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_parents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m                         \u001b[1;32myield\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_delta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado_env\\lib\\site-packages\\pgmpy\\estimators\\StructureScore.py\u001b[0m in \u001b[0;36mlocal_score\u001b[1;34m(self, variable, parents)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[0mvar_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mvar_cardinality\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mstate_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[0mnum_parents_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\doutorado_env\\lib\\site-packages\\pgmpy\\utils\\decorators.py\u001b[0m in \u001b[0;36m_convert_param_to_tuples\u001b[1;34m(obj, variable, parents, complete_samples_only)\u001b[0m\n\u001b[0;32m      4\u001b[0m     ):\n\u001b[0;32m      5\u001b[0m         \u001b[0mparents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplete_samples_only\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_param_to_tuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pais = 'alemanha'\n",
    "#initialize auxiliary variables\n",
    "k=1 #total days used\n",
    "target_variable = 'Emission'\n",
    "edges_possibilities = []\n",
    "edges_frequency = []\n",
    "timemodel = []\n",
    "timeinference = []\n",
    "forecast_values = pd.DataFrame()\n",
    "\n",
    "#read all available dates\n",
    "dates = get_all_dates(pais)\n",
    "dates = dates[0:7]\n",
    "\n",
    "#begin the forecast experiment\n",
    "for i in tqdm(dates):\n",
    "    #dataset to save de forecast values\n",
    "    forecast_aux = pd.DataFrame()\n",
    "    forecast_date = []\n",
    "    forecast_hour = []\n",
    "    forecast_v = []\n",
    "    #dataset to learn the model\n",
    "    data_learn = get_dataset(pais,i, i+timedelta(days = 0))\n",
    "    #structural learning with the dataset of day i\n",
    "    data_learn.drop(['Date','Hour'], axis = 1, inplace = True)\n",
    "    ti = time.time()\n",
    "    best_model = DM.EdgesModel(data_learn, FullOpt)\n",
    "    print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ae126",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-11-28T00:43:20.231Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [02:15<01:05, 32.70s/it]"
     ]
    }
   ],
   "source": [
    "pais = 'alemanha'\n",
    "#initialize auxiliary variables\n",
    "k=1 #total days used\n",
    "target_variable = 'Emission'\n",
    "edges_possibilities = []\n",
    "edges_frequency = []\n",
    "timemodel = []\n",
    "timeinference = []\n",
    "forecast_values = pd.DataFrame()\n",
    "\n",
    "#read all available dates\n",
    "dates = get_all_dates(pais)\n",
    "dates = dates[0:7]\n",
    "\n",
    "#begin the forecast experiment\n",
    "for i in tqdm(dates):\n",
    "    #dataset to save de forecast values\n",
    "    forecast_aux = pd.DataFrame()\n",
    "    forecast_date = []\n",
    "    forecast_hour = []\n",
    "    forecast_v = []\n",
    "    #dataset to learn the model\n",
    "    data_learn = get_dataset(pais,i, i+timedelta(days = 0))\n",
    "    #structural learning with the dataset of day i\n",
    "    data_learn.drop(['Date','Hour'], axis = 1, inplace = True)\n",
    "    ti = time.time()\n",
    "    best_model = DM.EdgesModel(data_learn, FullOpt)[0]\n",
    "    gc.collect()\n",
    "\n",
    "    #get the markov blanket\n",
    "    best_model = blanket(best_model, target_variable)\n",
    "\n",
    "    #update the edges frequencies\n",
    "    edges_possibilities, edges_frequency = update_edges_frequencies(best_model, edges_possibilities, edges_frequency)\n",
    "    del best_model\n",
    "\n",
    "    #update threshold and select the edges\n",
    "    edges = update_threshold_select_edges(k, edges_possibilities, edges_frequency)\n",
    "    if len(edges)==0:\n",
    "        edges.append(('Emission-1','Emission')) \n",
    "\n",
    "    tf = time.time()\n",
    "    timemodel.append(tf-ti)\n",
    "    #forecast initial in day 8 (fit from 01 until 07)\n",
    "    if i >= dates[6] and i+timedelta(days = 1) in dates:\n",
    "        #bins\n",
    "        bins = bins_values(pais)\n",
    "\n",
    "        #fit dataset (last 3 days)\n",
    "        fit_data = get_dataset(pais,i-timedelta(days = 2), i)\n",
    "        fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 2), i)\n",
    "\n",
    "        #predict data of the entire day\n",
    "        predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "        predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "\n",
    "        #detects independent variables\n",
    "        independentes=[]\n",
    "        for col in fit_data.columns:\n",
    "            if col not in list(map(lambda x: x[0],edges))+list(map(lambda x: x[1],edges)):\n",
    "                independentes.append(col)\n",
    "\n",
    "        #drop independent columns\n",
    "        fit_data.drop(independentes, axis=1, inplace = True)\n",
    "        predict_data_day.drop(independentes, axis=1, inplace = True)\n",
    "\n",
    "        #transform data in levels (limitation of PGMPY)\n",
    "        levels = {}\n",
    "        aux = fit_dataall.copy()\n",
    "        aux = aux.append(predict_dataall)\n",
    "        for var in aux.columns:\n",
    "            levels[var] = set(aux[var])\n",
    "            fit_dataall[var] = fit_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            predict_dataall[var] = predict_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            if var in fit_data.columns:\n",
    "                fit_data[var] = fit_data[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                predict_data_day[var] = predict_data_day[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "        predict_data_day = predict_data_day.astype(int)\n",
    "\n",
    "        #Using the edges, get the bayesian model object\n",
    "        model = BayesianModel(edges)\n",
    "\n",
    "        #aux\n",
    "        aux_fore = []\n",
    "\n",
    "        #predict each point of day i+1\n",
    "        ti_inf = time.time()\n",
    "        for h in range(len(predict_data_day)):\n",
    "            forecast_date.append(i+timedelta(days = 1))\n",
    "            forecast_hour.append(h)\n",
    "            predict_data = predict_data_day.iloc[[h]]\n",
    "            predictall = predict_dataall.iloc[[h]]\n",
    "            fit_datah = fit_data.loc[0:len(fit_data)-3+h] #tau = 3 (forecast horizon)\n",
    "\n",
    "            smote = RandomOverSampler(random_state = 42)\n",
    "            y = fit_datah[target_variable]\n",
    "            X = fit_datah.copy()\n",
    "            del X[target_variable]\n",
    "            Xc, yc = smote.fit_resample(X,y)\n",
    "            fit_datah = Xc\n",
    "            fit_datah[target_variable] = yc\n",
    "\n",
    "            #fit the bayesian model to get de CPTs\n",
    "            model.fit(fit_datah)\n",
    "            model.get_cpds(node = target_variable)\n",
    "\n",
    "            #drop all variable in time window T+1 (unknown values - future states)\n",
    "            for c in predict_data.columns:\n",
    "                if '-1' not in c:\n",
    "                    predict_data[c] = predictall[c+str('-1')]\n",
    "            del predict_data[target_variable]\n",
    "\n",
    "            #solve limitation of unknown level \n",
    "            for col in predict_data.columns:\n",
    "                predict_data[col][predict_data[col]>=len(set(fit_datah[col]))] = len(set(fit_datah[col]))-1\n",
    "            y_pred = model.predict(predict_data, n_jobs = 1)\n",
    "            y_pred[target_variable] = y_pred[target_variable].replace(np.arange(0,len(levels[target_variable])),levels[target_variable])\n",
    "            for v in y_pred[target_variable]:\n",
    "                aux_fore.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "            fit_data = fit_data.append(predict_data_day.loc[h-3:h-3]).reset_index(drop = True) \n",
    "        forecast_aux['Date'] = forecast_date\n",
    "        forecast_aux['Hour'] = forecast_hour\n",
    "        forecast_aux['Emissions Forecast'] = smooth(aux_fore,5)\n",
    "        real_value = real_values(pais, i)\n",
    "        forecast_aux[target_variable] = real_value[target_variable]\n",
    "        forecast_values = forecast_values.append(forecast_aux)\n",
    "        tf_inf = time.time()\n",
    "        timeinference.append(tf_inf-ti_inf)\n",
    "    k = k+1\n",
    "#save the results on postgres\n",
    "df_edges = pd.DataFrame()\n",
    "df_edges['edges'] = edges_possibilities\n",
    "df_edges['frequencia'] = edges_frequency\n",
    "df_edges['total days'] = k-1\n",
    "df_time_model = pd.DataFrame()\n",
    "df_time_model['tempo'] = timemodel\n",
    "df_time_inference = pd.DataFrame()\n",
    "df_time_inference['tempo'] = timeinference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01e2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b935613acb66c38ada86896ecd2c1ee112d390ac9871f93198d4ae2c0cafad7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
