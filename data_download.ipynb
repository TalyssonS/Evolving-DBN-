{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import ENTSOE\n",
    "import psycopg2 as pg\n",
    "import sqlalchemy as sq\n",
    "import logging\n",
    "logging.disable()\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "def get_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE AND RETURN THE SQLACHEMY ENGINE OBJECT\n",
    "    -----------\n",
    "    output: object\n",
    "        SQLACHEMY ENGINE OBJECT - POSTGRESQL DATABASE CONNECTION\n",
    "    '''\n",
    "    user = 'postgres'\n",
    "    password = 123\n",
    "    host = 'localhost'\n",
    "    port = 5432\n",
    "    database = 'postgres'\n",
    "    return sq.create_engine(url=\"postgresql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database))\n",
    "\n",
    "DOMAIN_MAPPINGS = {\n",
    "    'AL': '10YAL-KESH-----5',\n",
    "    'AT': '10YAT-APG------L',\n",
    "    'BA': '10YBA-JPCC-----D',\n",
    "    'BE': '10YBE----------2',\n",
    "    'BG': '10YCA-BULGARIA-R',\n",
    "    'BY': '10Y1001A1001A51S',\n",
    "    'CH': '10YCH-SWISSGRIDZ',\n",
    "    'CZ': '10YCZ-CEPS-----N',\n",
    "    'DE': '10Y1001A1001A83F',\n",
    "    'DEp': '10Y1001A1001A63L',\n",
    "    'DK': '10Y1001A1001A65H',\n",
    "    'EE': '10Y1001A1001A39I',\n",
    "    'ES': '10YES-REE------0',\n",
    "    'FI': '10YFI-1--------U',\n",
    "    'FR': '10YFR-RTE------C',\n",
    "    'GB': '10YGB----------A',\n",
    "    'GB-NIR': '10Y1001A1001A016',\n",
    "    'GR': '10YGR-HTSO-----Y',\n",
    "    'HR': '10YHR-HEP------M',\n",
    "    'HU': '10YHU-MAVIR----U',\n",
    "    'IE': '10YIE-1001A00010',\n",
    "    'IT': '10YIT-GRTN-----B',\n",
    "    'LT': '10YLT-1001A0008Q',\n",
    "    'LU': '10YLU-CEGEDEL-NQ',\n",
    "    'LV': '10YLV-1001A00074',\n",
    "    # 'MD': 'MD',\n",
    "    'ME': '10YCS-CG-TSO---S',\n",
    "    'MK': '10YMK-MEPSO----8',\n",
    "    'MT': '10Y1001A1001A93C',\n",
    "    'NL': '10YNL----------L',\n",
    "    'NO': '10YNO-0--------C',\n",
    "    'PL': '10YPL-AREA-----S',\n",
    "    'PT': '10YPT-REN------W',\n",
    "    'RO': '10YRO-TEL------P',\n",
    "    'RS': '10YCS-SERBIATSOV',\n",
    "    'RU': '10Y1001A1001A49F',\n",
    "    'RU-KGD': '10Y1001A1001A50U',\n",
    "    'SE': '10YSE-1--------K',\n",
    "    'SI': '10YSI-ELES-----O',\n",
    "    'SK': '10YSK-SEPS-----K',\n",
    "    'TR': '10YTR-TEIAS----W',\n",
    "    'UA': '10YUA-WEPS-----0'\n",
    "}\n",
    "\n",
    "BIDDING_ZONES = DOMAIN_MAPPINGS.copy()\n",
    "BIDDING_ZONES.update({\n",
    "    'DE': '10Y1001A1001A63L',  # DE-AT-LU\n",
    "    'LU': '10Y1001A1001A63L',  # DE-AT-LU\n",
    "    'IT-NORD': '10Y1001A1001A73I',\n",
    "    'IT-CNOR': '10Y1001A1001A70O',\n",
    "    'IT-CSUD': '10Y1001A1001A71M',\n",
    "    'IT-SUD': '10Y1001A1001A788',\n",
    "    'IT-FOGN': '10Y1001A1001A72K',\n",
    "    'IT-ROSN': '10Y1001A1001A77A',\n",
    "    'IT-BRNN': '10Y1001A1001A699',\n",
    "    'IT-PRGP': '10Y1001A1001A76C',\n",
    "    'IT-SARD': '10Y1001A1001A74G',\n",
    "    'IT-SICI': '10Y1001A1001A75E',\n",
    "    'NO-1': '10YNO-1--------2',\n",
    "    'NO-2': '10YNO-2--------T',\n",
    "    'NO-3': '10YNO-3--------J',\n",
    "    'NO-4': '10YNO-4--------9',\n",
    "    'NO-5': '10Y1001A1001A48H',\n",
    "    'SE-1': '10Y1001A1001A44P',\n",
    "    'SE-2': '10Y1001A1001A45N',\n",
    "    'SE-3': '10Y1001A1001A46L',\n",
    "    'SE-4': '10Y1001A1001A47J',\n",
    "    'DK-1': '10YDK-1--------W',\n",
    "    'DK-2': '10YDK-2--------M'\n",
    "})\n",
    "\n",
    "TIMEZONE_MAPPINGS = {\n",
    "    'AL': 'Europe/Tirane',\n",
    "    'AT': 'Europe/Vienna',\n",
    "    'BA': 'Europe/Sarajevo',\n",
    "    'BE': 'Europe/Brussels',\n",
    "    'BG': 'Europe/Sofia',\n",
    "    'BY': 'Europe/Minsk',\n",
    "    'CH': 'Europe/Zurich',\n",
    "    'CZ': 'Europe/Prague',\n",
    "    'DE': 'Europe/Berlin',\n",
    "    'DEp': 'Europe/Berlin',\n",
    "    'DK': 'Europe/Copenhagen',\n",
    "    'EE': 'Europe/Talinn',\n",
    "    'ES': 'Europe/Madrid',\n",
    "    'FI': 'Europe/Helsinki',\n",
    "    'FR': 'Europe/Paris',\n",
    "    'GB': 'Europe/London',\n",
    "    'GB-NIR': 'Europe/Belfast',\n",
    "    'GR': 'Europe/Athens',\n",
    "    'HR': 'Europe/Zagreb',\n",
    "    'HU': 'Europe/Budapest',\n",
    "    'IE': 'Europe/Dublin',\n",
    "    'IT': 'Europe/Rome',\n",
    "    'LT': 'Europe/Vilnius',\n",
    "    'LU': 'Europe/Luxembourg',\n",
    "    'LV': 'Europe/Riga',\n",
    "    # 'MD': 'MD',\n",
    "    'ME': 'Europe/Podgorica',\n",
    "    'MK': 'Europe/Skopje',\n",
    "    'MT': 'Europe/Malta',\n",
    "    'NL': 'Europe/Amsterdam',\n",
    "    'NO': 'Europe/Oslo',\n",
    "    'PL': 'Europe/Warsaw',\n",
    "    'PT': 'Europe/Lisbon',\n",
    "    'RO': 'Europe/Bucharest',\n",
    "    'RS': 'Europe/Belgrade',\n",
    "    'RU': 'Europe/Moscow',\n",
    "    'RU-KGD': 'Europe/Kaliningrad',\n",
    "    'SE': 'Europe/Stockholm',\n",
    "    'SI': 'Europe/Ljubljana',\n",
    "    'SK': 'Europe/Bratislava',\n",
    "    'TR': 'Europe/Istanbul',\n",
    "    'UA': 'Europe/Kiev'\n",
    "}\n",
    "\n",
    "PSRTYPE_MAPPINGS = {\n",
    "    'A03': 'Mixed',\n",
    "    'A04': 'Generation',\n",
    "    'A05': 'Load',\n",
    "    'B01': 'Biomass',\n",
    "    'B02': 'Fossil Brown coal/Lignite',\n",
    "    'B03': 'Fossil Coal-derived gas',\n",
    "    'B04': 'Fossil Gas',\n",
    "    'B05': 'Fossil Hard coal',\n",
    "    'B06': 'Fossil Oil',\n",
    "    'B07': 'Fossil Oil shale',\n",
    "    'B08': 'Fossil Peat',\n",
    "    'B09': 'Geothermal',\n",
    "    'B10': 'Hydro Pumped Storage',\n",
    "    'B11': 'Hydro Run-of-river and poundage',\n",
    "    'B12': 'Hydro Water Reservoir',\n",
    "    'B13': 'Marine',\n",
    "    'B14': 'Nuclear',\n",
    "    'B15': 'Other renewable',\n",
    "    'B16': 'Solar',\n",
    "    'B17': 'Waste',\n",
    "    'B18': 'Wind Offshore',\n",
    "    'B19': 'Wind Onshore',\n",
    "    'B20': 'Other',\n",
    "    'B21': 'AC Link',\n",
    "    'B22': 'DC Link',\n",
    "    'B23': 'Substation',\n",
    "    'B24': 'Transformer'}\n",
    "\n",
    "DOCSTATUS = {'A05': 'Active', 'A09': 'Cancelled', 'A13': 'Withdrawn'}\n",
    "\n",
    "BSNTYPE = {'A29': 'Already allocated capacity (AAC)',\n",
    "           'A43': 'Requested capacity (without price)',\n",
    "           'A46': 'System Operator redispatching',\n",
    "           'A53': 'Planned maintenance',\n",
    "           'A54': 'Unplanned outage',\n",
    "           'A85': 'Internal redispatch',\n",
    "           'A95': 'Frequency containment reserve',\n",
    "           'A96': 'Automatic frequency restoration reserve',\n",
    "           'A97': 'Manual frequency restoration reserve',\n",
    "           'A98': 'Replacement reserve',\n",
    "           'B01': 'Interconnector network evolution',\n",
    "           'B02': 'Interconnector network dismantling',\n",
    "           'B03': 'Counter trade',\n",
    "           'B04': 'Congestion costs',\n",
    "           'B05': 'Capacity allocated (including price)',\n",
    "           'B07': 'Auction revenue',\n",
    "           'B08': 'Total nominated capacity',\n",
    "           'B09': 'Net position',\n",
    "           'B10': 'Congestion income',\n",
    "           'B11': 'Production unit'}\n",
    "\n",
    "DOCUMENTTYPE = {'A09': 'Finalised schedule',\n",
    "                'A11': 'Aggregated energy data report',\n",
    "                'A25': 'Allocation result document',\n",
    "                'A26': 'Capacity document',\n",
    "                'A31': 'Agreed capacity',\n",
    "                'A44': 'Price Document',\n",
    "                'A61': 'Estimated Net Transfer Capacity',\n",
    "                'A63': 'Redispatch notice',\n",
    "                'A65': 'System total load',\n",
    "                'A68': 'Installed generation per type',\n",
    "                'A69': 'Wind and solar forecast',\n",
    "                'A70': 'Load forecast margin',\n",
    "                'A71': 'Generation forecast',\n",
    "                'A72': 'Reservoir filling information',\n",
    "                'A73': 'Actual generation',\n",
    "                'A74': 'Wind and solar generation',\n",
    "                'A75': 'Actual generation per type',\n",
    "                'A76': 'Load unavailability',\n",
    "                'A77': 'Production unavailability',\n",
    "                'A78': 'Transmission unavailability',\n",
    "                'A79': 'Offshore grid infrastructure unavailability',\n",
    "                'A80': 'Generation unavailability',\n",
    "                'A81': 'Contracted reserves',\n",
    "                'A82': 'Accepted offers',\n",
    "                'A83': 'Activated balancing quantities',\n",
    "                'A84': 'Activated balancing prices',\n",
    "                'A85': 'Imbalance prices',\n",
    "                'A86': 'Imbalance volume',\n",
    "                'A87': 'Financial situation',\n",
    "                'A88': 'Cross border balancing',\n",
    "                'A89': 'Contracted reserve prices',\n",
    "                'A90': 'Interconnection network expansion',\n",
    "                'A91': 'Counter trade notice',\n",
    "                'A92': 'Congestion costs',\n",
    "                'A93': 'DC link capacity',\n",
    "                'A94': 'Non EU allocations',\n",
    "                'A95': 'Configuration document',\n",
    "                'B11': 'Flow-based allocations'}\n",
    "\n",
    "def _extract_timeseries(xml_text):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    xml_text : str\n",
    "    Yields\n",
    "    -------\n",
    "    bs4.element.tag\n",
    "    \"\"\"\n",
    "    if not xml_text:\n",
    "        return\n",
    "    soup = bs4.BeautifulSoup(xml_text, 'html.parser')\n",
    "    for timeseries in soup.find_all('timeseries'):\n",
    "        yield timeseries\n",
    "\n",
    "def _parse_generation_forecast_timeseries(soup):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : bs4.element.tag\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "    \"\"\"\n",
    "    psrtype = soup.find('psrtype').text\n",
    "    positions = []\n",
    "    quantities = []\n",
    "    for point in soup.find_all('point'):\n",
    "        positions.append(int(point.find('position').text))\n",
    "        quantities.append(float(point.find('quantity').text))\n",
    "\n",
    "    series = pd.Series(index=positions, data=quantities)\n",
    "    series = series.sort_index()\n",
    "    series.index = _parse_datetimeindex(soup)\n",
    "\n",
    "    series.name = PSRTYPE_MAPPINGS[psrtype]\n",
    "    return series\n",
    "\n",
    "def _parse_datetimeindex(soup):\n",
    "    \"\"\"\n",
    "    Create a datetimeindex from a parsed beautifulsoup,\n",
    "    given that it contains the elements 'start', 'end'\n",
    "    and 'resolution'\n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : bs4.element.tag\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DatetimeIndex\n",
    "    \"\"\"\n",
    "    start = pd.Timestamp(soup.find('start').text)\n",
    "    end = pd.Timestamp(soup.find('end').text)\n",
    "    delta = _resolution_to_timedelta(res_text=soup.find('resolution').text)\n",
    "    index = pd.date_range(start=start, end=end, freq=delta, closed='left')\n",
    "    return index\n",
    "\n",
    "def _resolution_to_timedelta(res_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert an Entsoe resolution to something that pandas can understand\n",
    "    \"\"\"\n",
    "    resolutions = {\n",
    "        'PT60M': '60min',\n",
    "        'P1Y': '12M',\n",
    "        'PT15M': '15min',\n",
    "        'PT30M': '30min'\n",
    "    }\n",
    "    delta = resolutions.get(res_text)\n",
    "    if delta is None:\n",
    "        raise NotImplementedError(\"Sorry, I don't know what to do with the \"\n",
    "                                  \"resolution '{}', because there was no \"\n",
    "                                  \"documentation to be found of this format. \"\n",
    "                                  \"Everything is hard coded. Please open an \"\n",
    "                                  \"issue.\".format(res_text))\n",
    "    return delta\n",
    "\n",
    "def data_arrange(generation):\n",
    "    data_table = generation\n",
    "\n",
    "    set_columns = generation.columns\n",
    "    set_gen = ['Fossil Brown coal/Lignite', 'Fossil Coal-derived gas', 'Fossil Gas', 'Fossil Hard coal', 'Fossil Oil',\n",
    "               'Fossil Oil shale', 'Fossil Peat', 'Geothermal', 'Hydro Pumped Storage', 'Hydro Run-of-river and poundage',\n",
    "               'Hydro Water Reservoir', 'Marine', 'Nuclear', 'Other renewable', 'Solar', 'Waste', 'Wind Offshore', 'Wind Onshore', 'Other', 'Biomass']\n",
    "\n",
    "    for kind in set_gen:\n",
    "        if not kind in set_columns:\n",
    "            data_table[kind] = [0]*(data_table.shape[0])\n",
    "    \n",
    "    EFt = (71*data_table['Biomass']+800*(data_table['Fossil Hard coal'] + data_table['Fossil Coal-derived gas']) +400*data_table['Fossil Gas']+520*data_table['Fossil Oil']+520*data_table['Fossil Oil shale'] + 376*data_table['Fossil Peat'] + 45*data_table['Geothermal']+ 34*data_table['Hydro Pumped Storage'] + 4*data_table['Hydro Run-of-river and poundage']+\n",
    "    9.0*data_table['Hydro Water Reservoir']+  820*data_table['Fossil Brown coal/Lignite'] + 376*data_table['Marine'] + 11*data_table['Nuclear'] + 43*data_table['Solar'] + 690*data_table['Waste'] + \n",
    "    9*data_table['Wind Offshore'] + 8*data_table['Wind Onshore']+ 33*data_table['Other renewable'] + 376*data_table['Other'])/(data_table.sum(axis=1))\n",
    "\n",
    "    return EFt\n",
    "\n",
    "def _datetime_to_str(dtm):\n",
    "    \"\"\"\n",
    "    Convert a datetime object to a string in UTC\n",
    "    of the form YYYYMMDDhh00\n",
    "    Parameters\n",
    "    ----------\n",
    "    dtm : pd.Timestamp\n",
    "        Recommended to use a timezone-aware object!\n",
    "        If timezone-naive, UTC is assumed\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "    \"\"\"\n",
    "    if dtm.tzinfo is not None and dtm.tzinfo != pytz.UTC:\n",
    "        dtm = dtm.tz_convert(\"UTC\")\n",
    "    fmt = '%Y%m%d%H00'\n",
    "    ret_str = dtm.strftime(fmt)\n",
    "    return ret_str\n",
    "\n",
    "def base_request(params,api_key,start,end):\n",
    "    start_str = _datetime_to_str(start)\n",
    "    end_str = _datetime_to_str(end)\n",
    "    base_params = {\n",
    "        'securityToken': api_key,\n",
    "        'periodStart': start_str,\n",
    "        'periodEnd': end_str\n",
    "    }\n",
    "    params.update(base_params)\n",
    "    retry_count = 100\n",
    "    session = requests.Session()\n",
    "    error = None\n",
    "    for _ in range(retry_count):\n",
    "        response = session.get(url=URL,params=params)\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except requests.HTTPError as e:\n",
    "            error = e\n",
    "            soup = BeautifulSoup(response.text,'html.parser')\n",
    "            text = soup.find_all('text')\n",
    "            if len(text):\n",
    "                error_text = soup.find('text').text\n",
    "                if 'No matching data found' in error_text:\n",
    "                    return None\n",
    "            retry_delay = random.randint(1,180)\n",
    "            print(\"HTTP Error, retrying in {} seconds\".format(retry_delay))\n",
    "            sleep(retry_delay)\n",
    "        else:\n",
    "            return response\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1096it [00:00, 293941.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "def perdelta(start, end, delta):\n",
    "    curr = start\n",
    "    while curr < end:\n",
    "        yield curr\n",
    "        curr += delta\n",
    "days=[]\n",
    "for result in tqdm(perdelta(date(2019,1, 1), date(2022, 1, 1), timedelta(days=1))):\n",
    "    today=str(result)\n",
    "    today=today.replace('-','/')\n",
    "    t2=today\n",
    "    if today[5] == '0':\n",
    "        t2=str(t2[0:5])+str(t2[6:])\n",
    "        if today[8] == '0':\n",
    "            t2=str(t2[0:7])+str(t2[8:])\n",
    "    if today[8] == '0':\n",
    "        t2=str(t2[0:8])+str(t2[9:])\n",
    "    days.append(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paises = ['Germany', 'Portugal', 'Spain', 'Belgium']\n",
    "codigos = ['DE', 'PT','ES', 'BE']\n",
    "key = '' #Restful API access: use your key aunthentication.\n",
    "for p in range(len(paises)):\n",
    "    pais = paises[p]\n",
    "    cod = codigos[p]\n",
    "    datatotal= pd.DataFrame()\n",
    "    for d in tqdm(range(len(days)-1)):\n",
    "        \"\"\"until 25 march inc, put the desired date and +01:00; otherwise +02:00, until october 28 inc. SKIP 26/03 and 29/10\"\"\"\n",
    "        #query = ENTSOE.Entsoe('747061e5-91e4-4fc5-a722-659f33b144b6') #my key\n",
    "        datelist_start = pd.date_range(start=days[d]+str(' ')+str('00:00:00 +01:00'),end=days[d+1]+str(' ')+str('23:45:00 +01:00'),freq='D',name='datelist_start')\n",
    "        #period of interest. it cannot be too long, otherwise it will give you an error\n",
    "        filename = \"original_datasets/Data_2019_2021\"+str(pais)\n",
    "\n",
    "        realized = pd.DataFrame()\n",
    "        generation = pd.DataFrame()\n",
    "        emission = pd.DataFrame()\n",
    "        #session = requests.Session()\n",
    "        URL = 'https://transparency.entsoe.eu/api'\n",
    "        #print(\"\\n\")\n",
    "        query = ENTSOE.Entsoe(key) #my key\n",
    "        \"\"\"until 25 march inc, put the desired date and +01:00; otherwise +02:00, until october 28 inc. SKIP 26/03 and 29/10\"\"\"\n",
    "        consumption = []\n",
    "        #for k in datelist_start:\n",
    "        k=datelist_start[0]\n",
    "        #query.query_generation('DE',k,k+pd.DateOffset(hours=24),True) #country; split data every 24hours; parse the data\n",
    "        country_code = cod\n",
    "        start = k\n",
    "        end = k+pd.DateOffset(hours=24)\n",
    "        '''params = {\n",
    "            'documentType': 'A75',\n",
    "            'processType': 'A16',\n",
    "            'in_Domain': DOMAIN_MAPPINGS[country_code],\n",
    "            'securityToken': key,\n",
    "            'periodStart': start_str,\n",
    "            'periodEnd': end_str\n",
    "        }'''\n",
    "        params = {\n",
    "            'documentType': 'A75',\n",
    "            'processType': 'A16',\n",
    "            'in_Domain': DOMAIN_MAPPINGS[country_code],\n",
    "        }\n",
    "        #response = session.get(url=URL, params=params)\n",
    "        var = False\n",
    "        while var == False:\n",
    "            try:\n",
    "                response = base_request(params,key,start,end)\n",
    "                var = True\n",
    "            except:\n",
    "                var = False\n",
    "\n",
    "        #print(\"\\n\"+response.text+\"\\n\")\n",
    "        #if response is None:\n",
    "        #    print(\"Resposta é None\")\n",
    "        all_series = {}\n",
    "        for soup in _extract_timeseries(response.text):\n",
    "            ts = _parse_generation_forecast_timeseries(soup)\n",
    "            series = all_series.get(ts.name)\n",
    "            if series is None:\n",
    "                all_series[ts.name] = ts\n",
    "            else:\n",
    "                series = series.append(ts)\n",
    "                series.sort_index()\n",
    "                all_series[series.name] = series\n",
    "\n",
    "        for name in all_series:\n",
    "            ts = all_series[name]\n",
    "            all_series[name] = ts[~ts.index.duplicated(keep='first')]\n",
    "\n",
    "        gen_all = pd.DataFrame.from_dict(all_series)\n",
    "        gen_all = gen_all.tz_convert(TIMEZONE_MAPPINGS[country_code])\n",
    "        production = gen_all\n",
    "        #print(\"\\n\"+str(production.shape[0])+\"\\n\")\n",
    "        EF = data_arrange(production)   #calculate emission factors\n",
    "        gen_all = gen_all.resample('60T').mean()\n",
    "        EF = EF.resample('60T').mean()\n",
    "        generation = generation.append(gen_all)\n",
    "\n",
    "        emission = emission.append(pd.DataFrame(EF,columns=['Emission']))\n",
    "        query.query_consumption(cod, k, k + pd.DateOffset(hours = 24), True)\n",
    "        load = query.load.resample('60T').mean()\n",
    "        t=load.to_list()\n",
    "        consumption.extend(t)\n",
    "\n",
    "        #gen_sum = gen_all.sum(axis='columns') #total generation\n",
    "        #print(\".\",end=\"\")\n",
    "        #sys.stdout.flush()\n",
    "        generation.insert(20, \"Emission\",emission['Emission'], True) \n",
    "        generation['Consumption']=consumption\n",
    "        datatotal=datatotal.append(generation)\n",
    "    out_file = open(filename+\".pickle\",'wb')\n",
    "    datatotal['Date']=datatotal.index.strftime(\"%Y/%m/%d\")\n",
    "    datatotal['Hour']=datatotal.index.strftime(\"%H:%M\")\n",
    "    datatotal['UTC']=datatotal.index.strftime(\"%z\")\n",
    "    datatotal = datatotal.reset_index(drop = True)\n",
    "    pickle.dump(datatotal,out_file)\n",
    "    out_file.close()\n",
    "    datatotal.to_sql(name=str.lower(pais), con = get_connection(),schema = 'original_data', if_exists = 'replace', chunksize = None, index = False)\n",
    "    print(\"finish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
