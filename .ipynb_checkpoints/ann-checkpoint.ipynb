{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c48cbf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-13T19:51:45.674655Z",
     "start_time": "2022-11-13T19:51:39.650804Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import jupyter_contrib_nbextensions\n",
    "import random\n",
    "import warnings\n",
    "import sys \n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import psycopg2 as pg\n",
    "import sqlalchemy as sq\n",
    "import networkx as nx\n",
    "logging.disable()\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def errorf (real,forecast):\n",
    "    error=[]\n",
    "    for i in range(len(real)):\n",
    "        error.append(real[i]-forecast[i])\n",
    "    return error\n",
    "\n",
    "def open_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE\n",
    "    '''\n",
    "    conn = pg.connect(dbname='postgres', user = 'postgres', password = 123, host = 'localhost')\n",
    "    return conn\n",
    "def get_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE AND RETURN THE SQLACHEMY ENGINE OBJECT\n",
    "    -----------\n",
    "    output: object\n",
    "        SQLACHEMY ENGINE OBJECT - POSTGRESQL DATABASE CONNECTION\n",
    "    '''\n",
    "    user = 'postgres'\n",
    "    password = 123\n",
    "    host = 'localhost'\n",
    "    port = 5432\n",
    "    database = 'postgres'\n",
    "    return sq.create_engine(url=\"postgresql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database))\n",
    "\n",
    "\n",
    "def get_all_dates(pais):\n",
    "    q = '''select distinct cast(\"Date\" as DATE) as datas from pre_processed_data.dbn_features_selected_{pais} order by datas'''.format(pais=pais)\n",
    "    conn = open_connection()\n",
    "    date = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    datas = date['datas'].tolist()\n",
    "    return datas\n",
    "\n",
    "def get_dataset(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_features_selected_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "def get_dataset_allfeatures(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def bins_values(pais):\n",
    "    q = '''select \"Emission\" from pre_processed_data.bins_{pais} where \"Emission\" is not null'''.format(pais = pais)\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def real_values(pais, data):\n",
    "    q = '''select \"Emission\" from pre_processed_data.{pais} where \"Date\" = '{dataf}' '''.format(pais = pais, dataf = (data+timedelta(days = 1)).strftime(\"%Y/%m/%d\"))\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d70ebe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T08:42:16.944159Z",
     "start_time": "2022-11-13T19:51:45.676726Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1091/1091 [4:17:37<00:00, 14.17s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1080/1080 [2:53:56<00:00,  9.66s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1089/1089 [2:48:24<00:00,  9.28s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1090/1090 [2:50:31<00:00,  9.39s/it]\n"
     ]
    }
   ],
   "source": [
    "paises = ['Alemanha','Belgica','Espanha', 'Portugal']\n",
    "for pais in paises:\n",
    "    #initialize auxiliary variables\n",
    "    k=1 #total days used\n",
    "    target_variable = 'Emission'\n",
    "    #ann\n",
    "    timeinferenceann = []\n",
    "    forecast_valuesann = pd.DataFrame()\n",
    "\n",
    "    #read all available dates\n",
    "    dates = get_all_dates(pais)\n",
    "\n",
    "    for i in tqdm(dates):\n",
    "        if i >= dates[6] and i+timedelta(days = 1) in dates:\n",
    "            #bins\n",
    "            bins = bins_values(pais)\n",
    "\n",
    "            #fit dataset (last 7 days)\n",
    "            fit_data = get_dataset(pais,i-timedelta(days = 2), i)\n",
    "            fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 2), i)\n",
    "\n",
    "            #predict data of the entire day\n",
    "            predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "            predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "\n",
    "            #aux\n",
    "            aux_foreann = []\n",
    "            #dataset to save de forecast values\n",
    "            forecast_auxann = pd.DataFrame()\n",
    "            forecast_date = []\n",
    "            forecast_hour = []\n",
    "\n",
    "            #predict each point of day i+1\n",
    "            ti_inf = time.time()\n",
    "            for h in range(len(predict_data_day)):\n",
    "                forecast_date.append(i+timedelta(days = 1))\n",
    "                forecast_hour.append(h)\n",
    "                predict_data = predict_data_day.iloc[[h]]\n",
    "                predictall = predict_dataall.iloc[[h]]\n",
    "                fit_datah = fit_data.loc[0:len(fit_data)-3+h] #tau = 3 (forecast horizon)\n",
    "\n",
    "                #drop all variable in time window T+1 (unknown values - future states)\n",
    "                predict_data.drop(['Date', 'Hour'], axis = 1, inplace = True)\n",
    "                for c in predict_data.columns:\n",
    "                    if '-1' not in c:\n",
    "                        predict_data[c] = predictall[c+str('-1')]\n",
    "                del predict_data[target_variable]\n",
    "\n",
    "                #ANN model\n",
    "                clf = MLPClassifier()\n",
    "                X = fit_datah.copy()\n",
    "                y = fit_datah[[target_variable]]\n",
    "                X.drop([target_variable,'Date', 'Hour'], axis = 1, inplace = True)\n",
    "                clf.fit(X, y)\n",
    "                ann_predict = clf.predict(predict_data)\n",
    "                for v in ann_predict:\n",
    "                    aux_foreann.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "                fit_data = fit_data.append(predict_data_day.loc[h-3:h-3]).reset_index(drop = True) \n",
    "\n",
    "            forecast_auxann['Date'] = forecast_date\n",
    "            forecast_auxann['Hour'] = forecast_hour\n",
    "            forecast_auxann['Emissions Forecast'] = aux_foreann\n",
    "            real_value = real_values(pais, i)\n",
    "            forecast_auxann[target_variable] = real_value[target_variable]\n",
    "            forecast_valuesann = forecast_valuesann.append(forecast_auxann)\n",
    "            tf_inf = time.time()\n",
    "            timeinferenceann.append(tf_inf-ti_inf)\n",
    "        #save the results on postgres\n",
    "        df_time_inferenceann = pd.DataFrame()\n",
    "        df_time_inferenceann['tempo'] = timeinferenceann\n",
    "        df_time_inferenceann.to_sql(name='time_inference_'+str(pais)+str('ann'), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "        forecast_valuesann.to_sql(name='forecast_'+str(pais)+str('ann'), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8c35c6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-13T17:59:26.445674Z",
     "start_time": "2022-11-13T17:59:25.258977Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-7468323cd9b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mcatboost_narx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatboost_narx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sysidentpy.general_estimators import NARX\n",
    "from sysidentpy.basis_function import Polynomial\n",
    "from sysidentpy.utils.plotting import plot_residues_correlation, plot_results\n",
    "from sysidentpy.residues.residues_correlation import compute_residues_autocorrelation\n",
    "from sysidentpy.residues.residues_correlation import compute_cross_correlation\n",
    "\n",
    "\n",
    "basis_function=Polynomial(degree=1)\n",
    "\n",
    "catboost_narx = NARX(\n",
    "  base_estimator=CatBoostRegressor(\n",
    "    iterations=300,\n",
    "    learning_rate=0.1,\n",
    "    depth=6),\n",
    "  xlag=2,\n",
    "  ylag=2,\n",
    "  basis_function=basis_function,\n",
    "  fit_params={'verbose': False}\n",
    ")\n",
    "\n",
    "catboost_narx.fit(X.to_numpy(), y.to_numpy())\n",
    "yhat = catboost_narx.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a5de11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
