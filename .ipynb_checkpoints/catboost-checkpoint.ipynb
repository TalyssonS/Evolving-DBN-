{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c48cbf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-15T14:59:49.898097Z",
     "start_time": "2022-11-15T14:59:38.584292Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "#import jupyter_contrib_nbextensions\n",
    "import random\n",
    "import warnings\n",
    "import sys \n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import psycopg2 as pg\n",
    "import sqlalchemy as sq\n",
    "import networkx as nx\n",
    "logging.disable()\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def errorf (real,forecast):\n",
    "    error=[]\n",
    "    for i in range(len(real)):\n",
    "        error.append(real[i]-forecast[i])\n",
    "    return error\n",
    "\n",
    "def open_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE\n",
    "    '''\n",
    "    conn = pg.connect(dbname='postgres', user = 'postgres', password = 123, host = 'localhost')\n",
    "    return conn\n",
    "def get_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE AND RETURN THE SQLACHEMY ENGINE OBJECT\n",
    "    -----------\n",
    "    output: object\n",
    "        SQLACHEMY ENGINE OBJECT - POSTGRESQL DATABASE CONNECTION\n",
    "    '''\n",
    "    user = 'postgres'\n",
    "    password = 123\n",
    "    host = 'localhost'\n",
    "    port = 5432\n",
    "    database = 'postgres'\n",
    "    return sq.create_engine(url=\"postgresql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database))\n",
    "\n",
    "\n",
    "def get_all_dates(pais,horizonte):\n",
    "    q = '''select distinct cast(\"Date\" as DATE) as datas from pre_processed_data.dbn_features_selected_{pais}{horizonte} order by datas'''.format(pais=pais,horizonte=horizonte)\n",
    "    conn = open_connection()\n",
    "    date = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    datas = date['datas'].tolist()\n",
    "    return datas\n",
    "\n",
    "def get_dataset(pais,date_ini, date_fin,horizonte):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_features_selected_{pais}{horizonte} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin,horizonte=horizonte)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "def get_dataset_allfeatures(pais,date_ini, date_fin, horizonte):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_{pais}{horizonte} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin,horizonte=horizonte)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def bins_values(pais):\n",
    "    q = '''select \"Emission\" from pre_processed_data.bins_{pais} where \"Emission\" is not null'''.format(pais = pais)\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def real_values(pais, data):\n",
    "    q = '''select \"Emission\" from pre_processed_data.{pais} where \"Date\" = '{dataf}' '''.format(pais = pais, dataf = (data+timedelta(days = 1)).strftime(\"%Y/%m/%d\"))\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d70ebe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T12:42:13.184132Z",
     "start_time": "2022-11-15T14:59:49.900096Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1091/1091 [4:59:21<00:00, 16.46s/it]\n",
      "100%|█████████████████████████████████████| 1080/1080 [1:41:57<00:00,  5.66s/it]\n",
      "100%|█████████████████████████████████████| 1089/1089 [1:15:24<00:00,  4.15s/it]\n",
      "100%|█████████████████████████████████████| 1090/1090 [2:44:24<00:00,  9.05s/it]\n"
     ]
    }
   ],
   "source": [
    "paises = ['Alemanha','Belgica','Espanha', 'Portugal']\n",
    "horizonte = 3\n",
    "for pais in paises:\n",
    "    #initialize auxiliary variables\n",
    "    k=1 #total days used\n",
    "    target_variable = 'Emission'\n",
    "    #ann\n",
    "    timeinferenceann = []\n",
    "    forecast_valuesann = pd.DataFrame()\n",
    "\n",
    "    #read all available dates\n",
    "    dates = get_all_dates(pais,horizonte)\n",
    "\n",
    "    for i in tqdm(dates):\n",
    "        if i >= dates[6] and i+timedelta(days = 1) in dates:\n",
    "            #bins\n",
    "            bins = bins_values(pais)\n",
    "\n",
    "            #fit dataset (last 7 days)\n",
    "            fit_data = get_dataset(pais,i-timedelta(days = 6), i,horizonte)\n",
    "            fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 6), i, horizonte)\n",
    "\n",
    "            #predict data of the entire day\n",
    "            predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1),horizonte)\n",
    "            predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1),horizonte)\n",
    "\n",
    "            #aux\n",
    "            aux_foreann = []\n",
    "            #dataset to save de forecast values\n",
    "            forecast_auxann = pd.DataFrame()\n",
    "            forecast_date = []\n",
    "            forecast_hour = []\n",
    "\n",
    "            #predict each point of day i+1\n",
    "            ti_inf = time.time()\n",
    "            for h in range(len(predict_data_day)):\n",
    "                forecast_date.append(i+timedelta(days = 1))\n",
    "                forecast_hour.append(h)\n",
    "                predict_data = predict_data_day.iloc[[h]]\n",
    "                predictall = predict_dataall.iloc[[h]]\n",
    "                fit_datah = fit_data.loc[0:len(fit_data)-horizonte+h] #tau = horizonte (forecast horizon)\n",
    "\n",
    "                #drop all variable in time window T+1 (unknown values - future states)\n",
    "                predict_data.drop(['Date', 'Hour'], axis = 1, inplace = True)\n",
    "                for c in predict_data.columns:\n",
    "                    if '-1' not in c:\n",
    "                        predict_data[c] = predictall[c+str('-1')]\n",
    "                del predict_data[target_variable]\n",
    "\n",
    "                #ANN model\n",
    "                clf = MLPClassifier()\n",
    "                X = fit_datah.copy()\n",
    "                y = fit_datah[[target_variable]]\n",
    "                X.drop([target_variable,'Date', 'Hour'], axis = 1, inplace = True)\n",
    "                clf.fit(X, y)\n",
    "                ann_predict = clf.predict(predict_data)\n",
    "                for v in ann_predict:\n",
    "                    aux_foreann.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "                fit_data = fit_data.append(predict_data_day.loc[h-horizonte:h-horizonte]).reset_index(drop = True) \n",
    "\n",
    "            forecast_auxann['Date'] = forecast_date\n",
    "            forecast_auxann['Hour'] = forecast_hour\n",
    "            forecast_auxann['Emissions Forecast'] = aux_foreann\n",
    "            real_value = real_values(pais, i)\n",
    "            forecast_auxann[target_variable] = real_value[target_variable]\n",
    "            forecast_valuesann = forecast_valuesann.append(forecast_auxann)\n",
    "            tf_inf = time.time()\n",
    "            timeinferenceann.append(tf_inf-ti_inf)\n",
    "        #save the results on postgres\n",
    "        df_time_inferenceann = pd.DataFrame()\n",
    "        df_time_inferenceann['tempo'] = timeinferenceann\n",
    "        df_time_inferenceann.to_sql(name='time_inference_'+str(pais)+str('ann')+str(horizonte), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "        forecast_valuesann.to_sql(name='forecast_'+str(pais)+str('ann')+str(horizonte), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a5de11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
