{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dfa39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import HillClimbSearch\n",
    "from datetime import datetime, timedelta\n",
    "from pgmpy.inference import VariableElimination\n",
    "#import jupyter_contrib_nbextensions\n",
    "import random\n",
    "import warnings\n",
    "import sys \n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import psycopg2 as pg\n",
    "import sqlalchemy as sq\n",
    "import networkx as nx\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import BDeuScore\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import logging\n",
    "logging.disable()\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    vi = y[0]\n",
    "    vf = y[len(y)-1]\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    y_smooth[0] = vi\n",
    "    y_smooth[len(y_smooth)-1] = vf\n",
    "    return y_smooth\n",
    "\n",
    "def errorf (real,forecast):\n",
    "    error=[]\n",
    "    for i in range(len(real)):\n",
    "        error.append(real[i]-forecast[i])\n",
    "    return error\n",
    "\n",
    "def open_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE\n",
    "    '''\n",
    "    conn = pg.connect(dbname='postgres', user = 'postgres', password = 123, host = 'localhost')\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538c2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.281312442792967\n"
     ]
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_final_belgica\" fa '''\n",
    "conn = open_connection()\n",
    "forecast_values = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "print(median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49863e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.59248305917987"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_Belgicaann\" fa '''\n",
    "conn = open_connection()\n",
    "datasetln = pd.read_sql(q,conn)\n",
    "datasetln \n",
    "conn.close()\n",
    "median_absolute_error(datasetln['Emission'],datasetln['Emissions Forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47e0165c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.104894437510637"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_Portugalxgboost\" fa '''\n",
    "conn = open_connection()\n",
    "datasetln = pd.read_sql(q,conn)\n",
    "datasetln \n",
    "conn.close()\n",
    "median_absolute_error(datasetln['Emission'],datasetln['Emissions Forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b043b9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.071545718202437\n"
     ]
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_final_espanha\" fa '''\n",
    "conn = open_connection()\n",
    "forecast_values = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "print(median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50b1212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.969500794151479\n"
     ]
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_final_belgica\" fa '''\n",
    "conn = open_connection()\n",
    "forecast_values = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "print(median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b956062f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.76468869788097\n"
     ]
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_final_alemanha\" fa '''\n",
    "conn = open_connection()\n",
    "forecast_values = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "print(median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c1c875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.358640315443154\n"
     ]
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_final2_alemanha\" fa '''\n",
    "conn = open_connection()\n",
    "forecast_values = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "print(median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33893a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.358425700451278"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_Alemanhaann\" fa '''\n",
    "conn = open_connection()\n",
    "datasetln = pd.read_sql(q,conn)\n",
    "datasetln \n",
    "conn.close()\n",
    "median_absolute_error(datasetln['Emission'],datasetln['Emissions Forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28702fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7ab899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import HillClimbSearch\n",
    "from datetime import datetime, timedelta\n",
    "from pgmpy.inference import VariableElimination\n",
    "import random\n",
    "import warnings\n",
    "import sys \n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import psycopg2 as pg\n",
    "import sqlalchemy as sq\n",
    "import networkx as nx\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import BDeuScore \n",
    "from pgmpy.estimators import K2Score\n",
    "from pgmpy.estimators import BicScore\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "import logging\n",
    "logging.disable()\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    vi = y[0]\n",
    "    vf = y[len(y)-1]\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    y_smooth[0] = vi\n",
    "    y_smooth[len(y_smooth)-1] = vf\n",
    "    return y_smooth\n",
    "\n",
    "def errorf (real,forecast):\n",
    "    error=[]\n",
    "    for i in range(len(real)):\n",
    "        error.append(real[i]-forecast[i])\n",
    "    return error\n",
    "\n",
    "def open_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE\n",
    "    '''\n",
    "    conn = pg.connect(dbname='postgres', user = 'postgres', password = 123, host = 'localhost')\n",
    "    return conn\n",
    "def get_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE AND RETURN THE SQLACHEMY ENGINE OBJECT\n",
    "    -----------\n",
    "    output: object\n",
    "        SQLACHEMY ENGINE OBJECT - POSTGRESQL DATABASE CONNECTION\n",
    "    '''\n",
    "    user = 'postgres'\n",
    "    password = 123\n",
    "    host = 'localhost'\n",
    "    port = 5432\n",
    "    database = 'postgres'\n",
    "    return sq.create_engine(url=\"postgresql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database))\n",
    "\n",
    "def blanket(model,variable):\n",
    "    '''\n",
    "    Function to extract the Markov Blanket of the target variable (reduces the structure)\n",
    "    -----------\n",
    "    input:\n",
    "    model: list\n",
    "        list of edges\n",
    "        \n",
    "    variable: str\n",
    "        name of the target variable\n",
    "        \n",
    "    output:\n",
    "    blanket: list\n",
    "        list of edges \n",
    "    '''\n",
    "    blanket=[]\n",
    "    sons=[]\n",
    "    for i in model:\n",
    "        if i[0]==variable or i[1]==variable:\n",
    "            blanket.append(i)\n",
    "        if i[0]==variable:\n",
    "            sons.append(i[1])\n",
    "    for i in model:\n",
    "        if i[1] in sons and i[0]!=variable:\n",
    "            blanket.append(i)\n",
    "    return blanket\n",
    "\n",
    "def verifica_remove_ciclos(edges):\n",
    "    '''\n",
    "    Function to verify if the edges is a DAG and to try remove cycles\n",
    "    -----------\n",
    "    input:\n",
    "    edges: list\n",
    "        list of edges\n",
    "                \n",
    "    output:\n",
    "    blanket: list\n",
    "        list of edges \n",
    "    '''\n",
    "    edgesdag = edges #recebe o prÃ³prio modelo\n",
    "    #Verifica se tem ciclos e tenta remover invertendo uma aresta\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:  # (3) flip single edge\n",
    "            edges2 = edgesdag.copy()\n",
    "            edges2.extend([i[::-1]])\n",
    "            new_edges = edges2.copy()\n",
    "            new_edges.remove(i)\n",
    "            if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                edgesdag = new_edges.copy()\n",
    "                break\n",
    "    #Verifica se tem ciclos e tenta remover invertendo duas arestas\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:\n",
    "            for j in edgesdag:# (3) flip two edges\n",
    "                if i != j:\n",
    "                    edges2 = edgesdag.copy()\n",
    "                    edges2.extend([i[::-1]])\n",
    "                    edges2.extend([j[::-1]])\n",
    "                    new_edges = edges2.copy()\n",
    "                    new_edges.remove(i)\n",
    "                    new_edges.remove(j)\n",
    "                    if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                        edgesdag = new_edges.copy()\n",
    "                        breaker = True\n",
    "                        break\n",
    "            if breaker:\n",
    "                break\n",
    "    #Verifica se tem ciclos e tenta remover invertendo uma aresta e excluindo uma aresta\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:\n",
    "            for j in edgesdag:# (3) flip two edges\n",
    "                if i != j:\n",
    "                    edges2 = edgesdag.copy()\n",
    "                    edges2.extend([i[::-1]])\n",
    "                    new_edges = edges2.copy()\n",
    "                    new_edges.remove(i)\n",
    "                    new_edges.remove(j)\n",
    "                    if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                        edgesdag = new_edges.copy()\n",
    "                        breaker = True\n",
    "                        break\n",
    "            if breaker:\n",
    "                break\n",
    "    return edgesdag\n",
    "def get_all_dates(pais):\n",
    "    q = '''select distinct cast(\"Date\" as DATE) as datas from pre_processed_data.dbn_features_selected_{pais} order by datas'''.format(pais=pais)\n",
    "    conn = open_connection()\n",
    "    date = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    datas = date['datas'].tolist()\n",
    "    return datas\n",
    "\n",
    "def get_dataset(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_features_selected_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "def get_dataset_allfeatures(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "def update_edges_frequencies(best_model, edges_possibilities, edges_frequency):\n",
    "    if not edges_possibilities:\n",
    "        edges_possibilities = best_model\n",
    "        for p in range(len(edges_possibilities)):\n",
    "            edges_frequency.append(1)\n",
    "    else:\n",
    "        for v in range(len(best_model)):\n",
    "            if best_model[v] not in edges_possibilities:\n",
    "                edges_possibilities.append(best_model[v])\n",
    "                edges_frequency.append(1)\n",
    "            else:\n",
    "                for f in range(len(edges_possibilities)):\n",
    "                    if best_model[v] == edges_possibilities[f]:\n",
    "                        edges_frequency[f]=edges_frequency[f]+1\n",
    "    return edges_possibilities, edges_frequency\n",
    "\n",
    "def update_threshold_select_edges(k, edges_possibilities, edges_frequency):\n",
    "    fth = 1/3+np.sqrt(2/k)\n",
    "    if fth>0.4:\n",
    "        fth=0.4\n",
    "    edges_frequency_v=[edges_frequency[i]/k for i in range(len(edges_frequency))]\n",
    "    edges=[]\n",
    "    for i in range(len(edges_possibilities)):\n",
    "        if edges_frequency_v[i]>=fth and edges_possibilities[i] not in edges:\n",
    "            if edges_possibilities[i][::-1] not in edges_possibilities:\n",
    "                edges.append(edges_possibilities[i])\n",
    "            else: \n",
    "                if edges_frequency_v[i] > edges_frequency_v[edges_possibilities.index(edges_possibilities[i][::-1])]:\n",
    "                    edges.append(edges_possibilities[i])\n",
    "                else: \n",
    "                    edges.append(edges_possibilities[i][::-1])\n",
    "        elif edges_frequency_v[i]<fth:\n",
    "            for j in range(len(edges_possibilities)):\n",
    "                if edges_possibilities[i]==edges_possibilities[j][::-1]:\n",
    "                    if edges_frequency_v[i]+edges_frequency_v[j]>=fth:\n",
    "                        if edges_frequency_v[i]>edges_frequency_v[j]:\n",
    "                            edges.append(edges_possibilities[i])\n",
    "                        if edges_frequency_v[i]<edges_frequency_v[j]:\n",
    "                            edges.append(edges_possibilities[j])\n",
    "                        if edges_frequency_v[i]==edges_frequency_v[j]:\n",
    "                            auxci=0\n",
    "                            auxcj=0\n",
    "                            for s in range(len(edges)):\n",
    "                                if edges[s]==edges_possibilities[i]:\n",
    "                                    auxci=auxci+1\n",
    "                                if edges[s]==edges_possibilities[j]:\n",
    "                                    auxcj=auxcj+1\n",
    "                            if auxci>0:\n",
    "                                edges.append(edges_possibilities[i])\n",
    "                            elif auxcj>0:\n",
    "                                edges.append(edges_possibilities[j])\n",
    "                            else: \n",
    "                                import random\n",
    "                                edges.append(random.choice([edges_possibilities[i],edges_possibilities[j]]))\n",
    "    edges = list(set(edges))\n",
    "    return edges\n",
    "\n",
    "def bins_values(pais):\n",
    "    q = '''select \"Emission\" from pre_processed_data.bins_{pais} where \"Emission\" is not null'''.format(pais = pais)\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def real_values(pais, data):\n",
    "    q = '''select \"Emission\" from pre_processed_data.{pais} where \"Date\" = '{dataf}' '''.format(pais = pais, dataf = (data+timedelta(days = 1)).strftime(\"%Y/%m/%d\"))\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def main(pais,from_stop):\n",
    "    if from_stop == True:\n",
    "        target_variable = 'Emission'\n",
    "        conn = open_connection()\n",
    "        q = f'''select * from results.edges_frequency_final_{pais}'''\n",
    "        dados_arestas = pd.read_sql(q,conn)\n",
    "        edges_possibilities = dados_arestas['edges'].tolist()\n",
    "        edges_frequency = dados_arestas['frequencia'].tolist()\n",
    "        k = dados_arestas['total days'][0]\n",
    "        \n",
    "        q = f'''select * from results.forecast_final_{pais}'''\n",
    "        forecast_values = pd.read_sql(q,conn)\n",
    "        \n",
    "        q = f'''select * from results.time_model_final_{pais}'''\n",
    "        timemodel = pd.read_sql(q,conn)\n",
    "        timemodel = timemodel['tempo'].tolist()\n",
    "        \n",
    "        q = f'''select * from results.time_inference_final_{pais}'''\n",
    "        timeinference = pd.read_sql(q,conn)\n",
    "        timeinference = timeinference['tempo'].tolist()\n",
    "        conn.close()\n",
    "        \n",
    "    else: \n",
    "        #initialize auxiliary variables\n",
    "        k=1 #total days used\n",
    "        target_variable = 'Emission'\n",
    "        edges_possibilities = []\n",
    "        edges_frequency = []\n",
    "        timemodel = []\n",
    "        timeinference = []\n",
    "        forecast_values = pd.DataFrame()\n",
    "\n",
    "    #read all available dates\n",
    "    dates = get_all_dates(pais)[k-1:]\n",
    "\n",
    "    #begin the forecast experiment\n",
    "    for i in tqdm(dates):\n",
    "        #dataset to save de forecast values\n",
    "        forecast_aux = pd.DataFrame()\n",
    "        forecast_date = []\n",
    "        forecast_hour = []\n",
    "        forecast_v = []\n",
    "        #dataset to learn the model\n",
    "        data_learn = get_dataset(pais,i, i+timedelta(days = 0))\n",
    "        #structural learning with the dataset of day i\n",
    "        data_learn.drop(['Date','Hour'], axis = 1, inplace = True)\n",
    "        smote = SMOTE()\n",
    "        y = data_learn[target_variable]\n",
    "        X = data_learn.copy()\n",
    "        del X[target_variable]\n",
    "        Xc, yc = smote.fit_resample(X,y)\n",
    "        data_learn = Xc\n",
    "        data_learn[target_variable] = yc\n",
    "        \n",
    "        ti = time.time()\n",
    "        est = HillClimbSearch(data_learn)\n",
    "        best_model = est.estimate(scoring_method=BicScore(data_learn),  show_progress=False)\n",
    "        best_model = list(best_model.edges())\n",
    "        if len(best_model)==0:\n",
    "            best_model = [('Emission-1','Emission')]\n",
    "\n",
    "        #get the markov blanket\n",
    "        best_model = blanket(best_model, target_variable)\n",
    "\n",
    "        #update the edges frequencies\n",
    "        edges_possibilities, edges_frequency = update_edges_frequencies(best_model, edges_possibilities, edges_frequency)\n",
    "\n",
    "        #update threshold and select the edges\n",
    "        edges = update_threshold_select_edges(k, edges_possibilities, edges_frequency)\n",
    "        if len(edges)==0:\n",
    "            edges.append(('Emission-1','Emission')) \n",
    "\n",
    "        tf = time.time()\n",
    "        timemodel.append(tf-ti)\n",
    "        #forecast initial in day 8 (fit from 01 until 07)\n",
    "        \n",
    "        if from_stop == True:\n",
    "            inicio = dates[0]\n",
    "        else:\n",
    "            inicio = dates[6]\n",
    "        if i >= inicio and i+timedelta(days = 1) in dates:\n",
    "            #bins\n",
    "            bins = bins_values(pais)\n",
    "            \n",
    "            #fit dataset (last 3 days)\n",
    "            fit_data = get_dataset(pais,i-timedelta(days = 2), i)\n",
    "            fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 2), i)\n",
    "\n",
    "            #predict data of the entire day\n",
    "            predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "            predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "\n",
    "            #detects independent variables\n",
    "            independentes=[]\n",
    "            for col in fit_data.columns:\n",
    "                if col not in list(map(lambda x: x[0],edges))+list(map(lambda x: x[1],edges)):\n",
    "                    independentes.append(col)\n",
    "\n",
    "            #drop independent columns\n",
    "            fit_data.drop(independentes, axis=1, inplace = True)\n",
    "            predict_data_day.drop(independentes, axis=1, inplace = True)\n",
    "\n",
    "            #transform data in levels (limitation of PGMPY)\n",
    "            levels = {}\n",
    "            aux = fit_dataall.copy()\n",
    "            aux = aux.append(predict_dataall)\n",
    "            for var in aux.columns:\n",
    "                levels[var] = set(aux[var])\n",
    "                fit_dataall[var] = fit_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                predict_dataall[var] = predict_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                if var in fit_data.columns:\n",
    "                    fit_data[var] = fit_data[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                    predict_data_day[var] = predict_data_day[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            predict_data_day = predict_data_day.astype(int)\n",
    "            \n",
    "            #Using the edges, get the bayesian model object\n",
    "            model = BayesianNetwork(edges)\n",
    "\n",
    "            #aux\n",
    "            aux_fore = []\n",
    "        \n",
    "            #predict each point of day i+1\n",
    "            ti_inf = time.time()\n",
    "            for h in range(len(predict_data_day)):\n",
    "                forecast_date.append(i+timedelta(days = 1))\n",
    "                forecast_hour.append(h)\n",
    "                predict_data = predict_data_day.iloc[[h]]\n",
    "                predictall = predict_dataall.iloc[[h]]\n",
    "                fit_datah = fit_data.loc[0:len(fit_data)-3+h] #tau = 3 (forecast horizon)\n",
    "                \n",
    "                smote = SMOTE()\n",
    "                y = fit_datah[target_variable]\n",
    "                X = fit_datah.copy()\n",
    "                del X[target_variable]\n",
    "                Xc, yc = smote.fit_resample(X,y)\n",
    "                fit_datah = Xc\n",
    "                fit_datah[target_variable] = yc\n",
    "                \n",
    "                #fit the bayesian model to get de CPTs\n",
    "                model.fit(fit_datah,n_jobs = 1)\n",
    "                #model.get_cpds(node = target_variable)\n",
    "\n",
    "                #drop all variable in time window T+1 (unknown values - future states)\n",
    "                for c in predict_data.columns:\n",
    "                    if '-1' not in c:\n",
    "                        predict_data[c] = predictall[c+str('-1')]\n",
    "                del predict_data[target_variable]\n",
    "                \n",
    "                #solve limitation of unknown level \n",
    "                for col in predict_data.columns:\n",
    "                    predict_data[col][predict_data[col]>=len(set(fit_datah[col]))] = len(set(fit_datah[col]))-1\n",
    "                y_pred = model.predict(predict_data,n_jobs = 1)\n",
    "                y_pred[target_variable] = y_pred[target_variable].replace(np.arange(0,len(levels[target_variable])),levels[target_variable])\n",
    "                for v in y_pred[target_variable]:\n",
    "                    aux_fore.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "                fit_data = fit_data.append(predict_data_day.loc[h-3:h-3]).reset_index(drop = True) \n",
    "            forecast_aux['Date'] = forecast_date\n",
    "            forecast_aux['Hour'] = forecast_hour\n",
    "            forecast_aux['Emissions Forecast'] = smooth(aux_fore,3)\n",
    "            real_value = real_values(pais, i)\n",
    "            forecast_aux[target_variable] = real_value[target_variable]\n",
    "            forecast_values = forecast_values.append(forecast_aux)\n",
    "            tf_inf = time.time()\n",
    "            timeinference.append(tf_inf-ti_inf)\n",
    "        k = k+1\n",
    "    #save the results on postgres\n",
    "    df_edges = pd.DataFrame()\n",
    "    df_edges['edges'] = edges_possibilities\n",
    "    df_edges['frequencia'] = edges_frequency\n",
    "    df_edges['total days'] = k-1\n",
    "    df_time_model = pd.DataFrame()\n",
    "    df_time_model['tempo'] = timemodel\n",
    "    df_time_inference = pd.DataFrame()\n",
    "    df_time_inference['tempo'] = timeinference\n",
    "\n",
    "    df_edges.to_sql(name='edges_frequency_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    df_time_model.to_sql(name='time_model_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    df_time_inference.to_sql(name='time_inference_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    forecast_values.to_sql(name='forecast_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de479643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1091 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 5, n_neighbors = 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m X \u001b[38;5;241m=\u001b[39m data_learn\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m X[target_variable]\n\u001b[0;32m---> 52\u001b[0m Xc, yc \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m data_learn \u001b[38;5;241m=\u001b[39m Xc\n\u001b[1;32m     54\u001b[0m data_learn[target_variable] \u001b[38;5;241m=\u001b[39m yc\n",
      "File \u001b[0;32m~/anaconda3/envs/env_teste/lib/python3.9/site-packages/imblearn/base.py:83\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m     81\u001b[0m )\n\u001b[0;32m---> 83\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     86\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     89\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m~/anaconda3/envs/env_teste/lib/python3.9/site-packages/imblearn/over_sampling/_smote/filter.py:427\u001b[0m, in \u001b[0;36mSVMSMOTE._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    425\u001b[0m n_generated_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(fractions \u001b[38;5;241m*\u001b[39m (n_samples \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcount_nonzero(danger_bool) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 427\u001b[0m     nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_k_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupport_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdanger_bool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    432\u001b[0m     X_new_1, y_new_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_samples(\n\u001b[1;32m    433\u001b[0m         _safe_indexing(support_vector, np\u001b[38;5;241m.\u001b[39mflatnonzero(danger_bool)),\n\u001b[1;32m    434\u001b[0m         y\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m         step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcount_nonzero(safety_bool) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/env_teste/lib/python3.9/site-packages/sklearn/neighbors/_base.py:749\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    747\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    750\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected n_neighbors <= n_samples, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but n_samples = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, n_neighbors = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_samples_fit, n_neighbors)\n\u001b[1;32m    752\u001b[0m     )\n\u001b[1;32m    754\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    755\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 5, n_neighbors = 6"
     ]
    }
   ],
   "source": [
    "pais = 'alemanha'\n",
    "from_stop = False\n",
    "if from_stop == True:\n",
    "    target_variable = 'Emission'\n",
    "    conn = open_connection()\n",
    "    q = f'''select * from results.edges_frequency_final_{pais}'''\n",
    "    dados_arestas = pd.read_sql(q,conn)\n",
    "    edges_possibilities = dados_arestas['edges'].tolist()\n",
    "    edges_frequency = dados_arestas['frequencia'].tolist()\n",
    "    k = dados_arestas['total days'][0]\n",
    "\n",
    "    q = f'''select * from results.forecast_final_{pais}'''\n",
    "    forecast_values = pd.read_sql(q,conn)\n",
    "\n",
    "    q = f'''select * from results.time_model_final_{pais}'''\n",
    "    timemodel = pd.read_sql(q,conn)\n",
    "    timemodel = timemodel['tempo'].tolist()\n",
    "\n",
    "    q = f'''select * from results.time_inference_final_{pais}'''\n",
    "    timeinference = pd.read_sql(q,conn)\n",
    "    timeinference = timeinference['tempo'].tolist()\n",
    "    conn.close()\n",
    "\n",
    "else: \n",
    "    #initialize auxiliary variables\n",
    "    k=1 #total days used\n",
    "    target_variable = 'Emission'\n",
    "    edges_possibilities = []\n",
    "    edges_frequency = []\n",
    "    timemodel = []\n",
    "    timeinference = []\n",
    "    forecast_values = pd.DataFrame()\n",
    "\n",
    "#read all available dates\n",
    "dates = get_all_dates(pais)[k-1:]\n",
    "\n",
    "#begin the forecast experiment\n",
    "for i in tqdm(dates):\n",
    "    #dataset to save de forecast values\n",
    "    forecast_aux = pd.DataFrame()\n",
    "    forecast_date = []\n",
    "    forecast_hour = []\n",
    "    forecast_v = []\n",
    "    #dataset to learn the model\n",
    "    data_learn = get_dataset(pais,i, i+timedelta(days = 0))\n",
    "    #structural learning with the dataset of day i\n",
    "    data_learn.drop(['Date','Hour'], axis = 1, inplace = True)\n",
    "    smote = SVMSMOTE()\n",
    "    y = data_learn[target_variable]\n",
    "    X = data_learn.copy()\n",
    "    del X[target_variable]\n",
    "    Xc, yc = smote.fit_resample(X,y)\n",
    "    data_learn = Xc\n",
    "    data_learn[target_variable] = yc\n",
    "\n",
    "    ti = time.time()\n",
    "    est = HillClimbSearch(data_learn)\n",
    "    best_model = est.estimate(scoring_method=BicScore(data_learn),  show_progress=False)\n",
    "    best_model = list(best_model.edges())\n",
    "    if len(best_model)==0:\n",
    "        best_model = [('Emission-1','Emission')]\n",
    "\n",
    "    #get the markov blanket\n",
    "    best_model = blanket(best_model, target_variable)\n",
    "\n",
    "    #update the edges frequencies\n",
    "    edges_possibilities, edges_frequency = update_edges_frequencies(best_model, edges_possibilities, edges_frequency)\n",
    "\n",
    "    #update threshold and select the edges\n",
    "    edges = update_threshold_select_edges(k, edges_possibilities, edges_frequency)\n",
    "    if len(edges)==0:\n",
    "        edges.append(('Emission-1','Emission')) \n",
    "\n",
    "    tf = time.time()\n",
    "    timemodel.append(tf-ti)\n",
    "    #forecast initial in day 8 (fit from 01 until 07)\n",
    "\n",
    "    if from_stop == True:\n",
    "        inicio = dates[0]\n",
    "    else:\n",
    "        inicio = dates[6]\n",
    "    if i >= inicio and i+timedelta(days = 1) in dates:\n",
    "        #bins\n",
    "        bins = bins_values(pais)\n",
    "\n",
    "        #fit dataset (last 3 days)\n",
    "        fit_data = get_dataset(pais,i-timedelta(days = 2), i)\n",
    "        fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 2), i)\n",
    "\n",
    "        #predict data of the entire day\n",
    "        predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "        predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "\n",
    "        #detects independent variables\n",
    "        independentes=[]\n",
    "        for col in fit_data.columns:\n",
    "            if col not in list(map(lambda x: x[0],edges))+list(map(lambda x: x[1],edges)):\n",
    "                independentes.append(col)\n",
    "\n",
    "        #drop independent columns\n",
    "        fit_data.drop(independentes, axis=1, inplace = True)\n",
    "        predict_data_day.drop(independentes, axis=1, inplace = True)\n",
    "\n",
    "        #transform data in levels (limitation of PGMPY)\n",
    "        levels = {}\n",
    "        aux = fit_dataall.copy()\n",
    "        aux = aux.append(predict_dataall)\n",
    "        for var in aux.columns:\n",
    "            levels[var] = set(aux[var])\n",
    "            fit_dataall[var] = fit_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            predict_dataall[var] = predict_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            if var in fit_data.columns:\n",
    "                fit_data[var] = fit_data[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                predict_data_day[var] = predict_data_day[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "        predict_data_day = predict_data_day.astype(int)\n",
    "\n",
    "        #Using the edges, get the bayesian model object\n",
    "        model = BayesianNetwork(edges)\n",
    "\n",
    "        #aux\n",
    "        aux_fore = []\n",
    "\n",
    "        #predict each point of day i+1\n",
    "        ti_inf = time.time()\n",
    "        for h in range(len(predict_data_day)):\n",
    "            forecast_date.append(i+timedelta(days = 1))\n",
    "            forecast_hour.append(h)\n",
    "            predict_data = predict_data_day.iloc[[h]]\n",
    "            predictall = predict_dataall.iloc[[h]]\n",
    "            fit_datah = fit_data.loc[0:len(fit_data)-3+h] #tau = 3 (forecast horizon)\n",
    "\n",
    "            smote = SVMSMOTE()\n",
    "            y = fit_datah[target_variable]\n",
    "            X = fit_datah.copy()\n",
    "            del X[target_variable]\n",
    "            Xc, yc = smote.fit_resample(X,y)\n",
    "            fit_datah = Xc\n",
    "            fit_datah[target_variable] = yc\n",
    "\n",
    "            #fit the bayesian model to get de CPTs\n",
    "            model.fit(fit_datah,n_jobs = 1)\n",
    "            #model.get_cpds(node = target_variable)\n",
    "\n",
    "            #drop all variable in time window T+1 (unknown values - future states)\n",
    "            for c in predict_data.columns:\n",
    "                if '-1' not in c:\n",
    "                    predict_data[c] = predictall[c+str('-1')]\n",
    "            del predict_data[target_variable]\n",
    "\n",
    "            #solve limitation of unknown level \n",
    "            for col in predict_data.columns:\n",
    "                predict_data[col][predict_data[col]>=len(set(fit_datah[col]))] = len(set(fit_datah[col]))-1\n",
    "            y_pred = model.predict(predict_data,n_jobs = 1)\n",
    "            y_pred[target_variable] = y_pred[target_variable].replace(np.arange(0,len(levels[target_variable])),levels[target_variable])\n",
    "            for v in y_pred[target_variable]:\n",
    "                aux_fore.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "            fit_data = fit_data.append(predict_data_day.loc[h-3:h-3]).reset_index(drop = True) \n",
    "        forecast_aux['Date'] = forecast_date\n",
    "        forecast_aux['Hour'] = forecast_hour\n",
    "        forecast_aux['Emissions Forecast'] = smooth(aux_fore,3)\n",
    "        real_value = real_values(pais, i)\n",
    "        forecast_aux[target_variable] = real_value[target_variable]\n",
    "        forecast_values = forecast_values.append(forecast_aux)\n",
    "        tf_inf = time.time()\n",
    "        timeinference.append(tf_inf-ti_inf)\n",
    "    k = k+1\n",
    "#save the results on postgres\n",
    "df_edges = pd.DataFrame()\n",
    "df_edges['edges'] = edges_possibilities\n",
    "df_edges['frequencia'] = edges_frequency\n",
    "df_edges['total days'] = k-1\n",
    "df_time_model = pd.DataFrame()\n",
    "df_time_model['tempo'] = timemodel\n",
    "df_time_inference = pd.DataFrame()\n",
    "df_time_inference['tempo'] = timeinference\n",
    "\n",
    "df_edges.to_sql(name='edges_frequency_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "df_time_model.to_sql(name='time_model_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "df_time_inference.to_sql(name='time_inference_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "forecast_values.to_sql(name='forecast_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50bf3a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emission</th>\n",
       "      <th>Emission-1</th>\n",
       "      <th>Lignite</th>\n",
       "      <th>Lignite-1</th>\n",
       "      <th>Hard coal</th>\n",
       "      <th>Hard coal-1</th>\n",
       "      <th>W. Onshore</th>\n",
       "      <th>Fossil Gas</th>\n",
       "      <th>W. Onshore-1</th>\n",
       "      <th>Fossil Gas-1</th>\n",
       "      <th>Nuclear</th>\n",
       "      <th>Nuclear-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Emission  Emission-1  Lignite  Lignite-1  Hard coal  Hard coal-1  \\\n",
       "0          8           3        7          1          4            7   \n",
       "1          6           3        5          1          3            6   \n",
       "2          6           3        5          1          3            6   \n",
       "3          6           8        5          7          3            4   \n",
       "4          5           6        4          5          3            3   \n",
       "5          5           6        4          5          3            3   \n",
       "6          4           6        4          5          3            3   \n",
       "7          4           5        4          4          3            3   \n",
       "8          4           5        4          4          3            3   \n",
       "9          4           4        4          4          4            3   \n",
       "10         4           4        4          4          4            3   \n",
       "11         4           4        4          4          4            3   \n",
       "12         4           4        4          4          4            4   \n",
       "13         4           4        4          4          4            4   \n",
       "14         4           4        4          4          4            4   \n",
       "15         4           4        4          4          4            4   \n",
       "16         3           4        4          4          4            4   \n",
       "17         3           4        4          4          4            4   \n",
       "18         3           4        4          4          4            4   \n",
       "19         3           3        4          4          4            4   \n",
       "20         3           3        4          4          4            4   \n",
       "21         4           3        4          4          4            4   \n",
       "22         4           3        4          4          4            4   \n",
       "23         4           3        4          4          4            4   \n",
       "\n",
       "    W. Onshore  Fossil Gas  W. Onshore-1  Fossil Gas-1  Nuclear  Nuclear-1  \n",
       "0           17           5            27             4       33         20  \n",
       "1           19           4            27             4       31         20  \n",
       "2           20           3            26             4       27         20  \n",
       "3           21           3            17             5       23         33  \n",
       "4           23           3            19             4       22         31  \n",
       "5           24           3            20             3       22         27  \n",
       "6           26           3            21             3       22         23  \n",
       "7           26           4            23             3       22         22  \n",
       "8           27           4            24             3       22         22  \n",
       "9           27           4            26             3       21         22  \n",
       "10          28           4            26             4       19         22  \n",
       "11          29           4            27             4       20         22  \n",
       "12          30           4            27             4       21         21  \n",
       "13          30           4            28             4       21         19  \n",
       "14          31           4            29             4       21         20  \n",
       "15          31           4            30             4       23         21  \n",
       "16          32           4            30             4       24         21  \n",
       "17          32           4            31             4       25         21  \n",
       "18          32           4            31             4       24         23  \n",
       "19          32           4            32             4       22         24  \n",
       "20          32           4            32             4       19         25  \n",
       "21          32           4            32             4       16         24  \n",
       "22          31           4            32             4       16         22  \n",
       "23          31           4            32             4       15         19  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bdeabc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
