{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfd0009c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-15T01:09:18.336990Z",
     "start_time": "2022-11-15T01:09:18.246194Z"
    }
   },
   "outputs": [],
   "source": [
    "import DefModules as DM\n",
    "from datetime import datetime, timedelta\n",
    "FullOpt =  True\n",
    "from pgmpy.inference import VariableElimination\n",
    "import jupyter_contrib_nbextensions\n",
    "import random\n",
    "import warnings\n",
    "import sys \n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import psycopg2 as pg\n",
    "import sqlalchemy as sq\n",
    "import networkx as nx\n",
    "logging.disable()\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import HillClimbSearch\n",
    "FullOpt =  True #True é para usar HC e false busca exaustiva\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import BdeuScore\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    vi = y[0]\n",
    "    vf = y[len(y)-1]\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    y_smooth[0] = vi\n",
    "    y_smooth[len(y_smooth)-1] = vf\n",
    "    return y_smooth\n",
    "\n",
    "def errorf (real,forecast):\n",
    "    error=[]\n",
    "    for i in range(len(real)):\n",
    "        error.append(real[i]-forecast[i])\n",
    "    return error\n",
    "\n",
    "def open_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE\n",
    "    '''\n",
    "    conn = pg.connect(dbname='postgres', user = 'postgres', password = 123, host = 'localhost')\n",
    "    return conn\n",
    "def get_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE AND RETURN THE SQLACHEMY ENGINE OBJECT\n",
    "    -----------\n",
    "    output: object\n",
    "        SQLACHEMY ENGINE OBJECT - POSTGRESQL DATABASE CONNECTION\n",
    "    '''\n",
    "    user = 'postgres'\n",
    "    password = 123\n",
    "    host = 'localhost'\n",
    "    port = 5432\n",
    "    database = 'postgres'\n",
    "    return sq.create_engine(url=\"postgresql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database))\n",
    "\n",
    "def blanket(model,variable):\n",
    "    '''\n",
    "    Function to extract the Markov Blanket of the target variable (reduces the structure)\n",
    "    -----------\n",
    "    input:\n",
    "    model: list\n",
    "        list of edges\n",
    "        \n",
    "    variable: str\n",
    "        name of the target variable\n",
    "        \n",
    "    output:\n",
    "    blanket: list\n",
    "        list of edges \n",
    "    '''\n",
    "    blanket=[]\n",
    "    sons=[]\n",
    "    for i in model:\n",
    "        if i[0]==variable or i[1]==variable:\n",
    "            blanket.append(i)\n",
    "        if i[0]==variable:\n",
    "            sons.append(i[1])\n",
    "    for i in model:\n",
    "        if i[1] in sons and i[0]!=variable:\n",
    "            blanket.append(i)\n",
    "    return blanket\n",
    "\n",
    "def verifica_remove_ciclos(edges):\n",
    "    '''\n",
    "    Function to verify if the edges is a DAG and to try remove cycles\n",
    "    -----------\n",
    "    input:\n",
    "    edges: list\n",
    "        list of edges\n",
    "                \n",
    "    output:\n",
    "    blanket: list\n",
    "        list of edges \n",
    "    '''\n",
    "    edgesdag = edges #recebe o próprio modelo\n",
    "    #Verifica se tem ciclos e tenta remover invertendo uma aresta\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:  # (3) flip single edge\n",
    "            edges2 = edgesdag.copy()\n",
    "            edges2.extend([i[::-1]])\n",
    "            new_edges = edges2.copy()\n",
    "            new_edges.remove(i)\n",
    "            if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                edgesdag = new_edges.copy()\n",
    "                break\n",
    "    #Verifica se tem ciclos e tenta remover invertendo duas arestas\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:\n",
    "            for j in edgesdag:# (3) flip two edges\n",
    "                if i != j:\n",
    "                    edges2 = edgesdag.copy()\n",
    "                    edges2.extend([i[::-1]])\n",
    "                    edges2.extend([j[::-1]])\n",
    "                    new_edges = edges2.copy()\n",
    "                    new_edges.remove(i)\n",
    "                    new_edges.remove(j)\n",
    "                    if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                        edgesdag = new_edges.copy()\n",
    "                        breaker = True\n",
    "                        break\n",
    "            if breaker:\n",
    "                break\n",
    "    #Verifica se tem ciclos e tenta remover invertendo uma aresta e excluindo uma aresta\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:\n",
    "            for j in edgesdag:# (3) flip two edges\n",
    "                if i != j:\n",
    "                    edges2 = edgesdag.copy()\n",
    "                    edges2.extend([i[::-1]])\n",
    "                    new_edges = edges2.copy()\n",
    "                    new_edges.remove(i)\n",
    "                    new_edges.remove(j)\n",
    "                    if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                        edgesdag = new_edges.copy()\n",
    "                        breaker = True\n",
    "                        break\n",
    "            if breaker:\n",
    "                break\n",
    "    return edgesdag\n",
    "def get_all_dates(pais):\n",
    "    q = '''select distinct cast(\"Date\" as DATE) as datas from pre_processed_data.dbn_features_selected_{pais} order by datas'''.format(pais=pais)\n",
    "    conn = open_connection()\n",
    "    date = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    datas = date['datas'].tolist()\n",
    "    return datas\n",
    "\n",
    "def get_dataset(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_features_selected_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "def get_dataset_allfeatures(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "def update_edges_frequencies(best_model, edges_possibilities, edges_frequency):\n",
    "    if not edges_possibilities:\n",
    "        edges_possibilities = best_model\n",
    "        for p in range(len(edges_possibilities)):\n",
    "            edges_frequency.append(1)\n",
    "    else:\n",
    "        for v in range(len(best_model)):\n",
    "            if best_model[v] not in edges_possibilities:\n",
    "                edges_possibilities.append(best_model[v])\n",
    "                edges_frequency.append(1)\n",
    "            else:\n",
    "                for f in range(len(edges_possibilities)):\n",
    "                    if best_model[v] == edges_possibilities[f]:\n",
    "                        edges_frequency[f]=edges_frequency[f]+1\n",
    "    return edges_possibilities, edges_frequency\n",
    "\n",
    "def update_threshold_select_edges(k, edges_possibilities, edges_frequency):\n",
    "    fth = 1/3+np.sqrt(2/k)\n",
    "    if fth>0.4:\n",
    "        fth=0.4\n",
    "    edges_frequency_v=[edges_frequency[i]/k for i in range(len(edges_frequency))]\n",
    "    edges=[]\n",
    "    for i in range(len(edges_possibilities)):\n",
    "        if edges_frequency_v[i]>=fth and edges_possibilities[i] not in edges:\n",
    "            if edges_possibilities[i][::-1] not in edges_possibilities:\n",
    "                edges.append(edges_possibilities[i])\n",
    "            else: \n",
    "                if edges_frequency_v[i] > edges_frequency_v[edges_possibilities.index(edges_possibilities[i][::-1])]:\n",
    "                    edges.append(edges_possibilities[i])\n",
    "                else: \n",
    "                    edges.append(edges_possibilities[i][::-1])\n",
    "        elif edges_frequency_v[i]<fth:\n",
    "            for j in range(len(edges_possibilities)):\n",
    "                if edges_possibilities[i]==edges_possibilities[j][::-1]:\n",
    "                    if edges_frequency_v[i]+edges_frequency_v[j]>=fth:\n",
    "                        if edges_frequency_v[i]>edges_frequency_v[j]:\n",
    "                            edges.append(edges_possibilities[i])\n",
    "                        if edges_frequency_v[i]<edges_frequency_v[j]:\n",
    "                            edges.append(edges_possibilities[j])\n",
    "                        if edges_frequency_v[i]==edges_frequency_v[j]:\n",
    "                            auxci=0\n",
    "                            auxcj=0\n",
    "                            for s in range(len(edges)):\n",
    "                                if edges[s]==edges_possibilities[i]:\n",
    "                                    auxci=auxci+1\n",
    "                                if edges[s]==edges_possibilities[j]:\n",
    "                                    auxcj=auxcj+1\n",
    "                            if auxci>0:\n",
    "                                edges.append(edges_possibilities[i])\n",
    "                            elif auxcj>0:\n",
    "                                edges.append(edges_possibilities[j])\n",
    "                            else: \n",
    "                                import random\n",
    "                                edges.append(random.choice([edges_possibilities[i],edges_possibilities[j]]))\n",
    "    edges = list(set(edges))\n",
    "    return edges\n",
    "\n",
    "def bins_values(pais):\n",
    "    q = '''select \"Emission\" from pre_processed_data.bins_{pais} where \"Emission\" is not null'''.format(pais = pais)\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def real_values(pais, data):\n",
    "    q = '''select \"Emission\" from pre_processed_data.{pais} where \"Date\" = '{dataf}' '''.format(pais = pais, dataf = (data+timedelta(days = 1)).strftime(\"%Y/%m/%d\"))\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def main(pais):\n",
    "    #initialize auxiliary variables\n",
    "    k=1 #total days used\n",
    "    target_variable = 'Emission'\n",
    "    edges_possibilities = []\n",
    "    edges_frequency = []\n",
    "    timemodel = []\n",
    "    timeinference = []\n",
    "    forecast_values = pd.DataFrame()\n",
    "\n",
    "    #read all available dates\n",
    "    dates = get_all_dates(pais)\n",
    "\n",
    "    #begin the forecast experiment\n",
    "    for i in tqdm(dates):\n",
    "        #dataset to save de forecast values\n",
    "        forecast_aux = pd.DataFrame()\n",
    "        forecast_date = []\n",
    "        forecast_hour = []\n",
    "        forecast_v = []\n",
    "        #dataset to learn the model\n",
    "        data_learn = get_dataset(pais,i, i+timedelta(days = 0))\n",
    "        #structural learning with the dataset of day i\n",
    "        data_learn.drop(['Date','Hour'], axis = 1, inplace = True)\n",
    "        ti = time.time()\n",
    "        best_model = DM.EdgesModel(data_learn, FullOpt)[0]\n",
    "\n",
    "        #get the markov blanket\n",
    "        best_model = blanket(best_model, target_variable)\n",
    "\n",
    "        #update the edges frequencies\n",
    "        edges_possibilities, edges_frequency = update_edges_frequencies(best_model, edges_possibilities, edges_frequency)\n",
    "\n",
    "        #update threshold and select the edges\n",
    "        edges = update_threshold_select_edges(k, edges_possibilities, edges_frequency)\n",
    "        if len(edges)==0:\n",
    "            edges.append(('Emission-1','Emission')) \n",
    "\n",
    "        tf = time.time()\n",
    "        timemodel.append(tf-ti)\n",
    "        #forecast initial in day 8 (fit from 01 until 07)\n",
    "        if i >= dates[6] and i+timedelta(days = 1) in dates:\n",
    "            #bins\n",
    "            bins = bins_values(pais)\n",
    "            \n",
    "            #fit dataset (last 7 days)\n",
    "            fit_data = get_dataset(pais,i-timedelta(days = 6), i)\n",
    "            fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 6), i)\n",
    "\n",
    "            #predict data of the entire day\n",
    "            predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "            predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "\n",
    "            #detects independent variables\n",
    "            independentes=[]\n",
    "            for col in fit_data.columns:\n",
    "                if col not in list(map(lambda x: x[0],edges))+list(map(lambda x: x[1],edges)):\n",
    "                    independentes.append(col)\n",
    "\n",
    "            #drop independent columns\n",
    "            fit_data.drop(independentes, axis=1, inplace = True)\n",
    "            predict_data_day.drop(independentes, axis=1, inplace = True)\n",
    "\n",
    "            #transform data in levels (limitation of PGMPY)\n",
    "            levels = {}\n",
    "            aux = fit_dataall.copy()\n",
    "            aux = aux.append(predict_dataall)\n",
    "            for var in aux.columns:\n",
    "                levels[var] = set(aux[var])\n",
    "                fit_dataall[var] = fit_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                predict_dataall[var] = predict_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                if var in fit_data.columns:\n",
    "                    fit_data[var] = fit_data[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                    predict_data_day[var] = predict_data_day[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            predict_data_day = predict_data_day.astype(int)\n",
    "            \n",
    "            #Using the edges, get the bayesian model object\n",
    "            model = BayesianModel(edges)\n",
    "\n",
    "            #aux\n",
    "            aux_fore = []\n",
    "        \n",
    "            #predict each point of day i+1\n",
    "            ti_inf = time.time()\n",
    "            for h in range(len(predict_data_day)):\n",
    "                forecast_date.append(i+timedelta(days = 1))\n",
    "                forecast_hour.append(h)\n",
    "                predict_data = predict_data_day.iloc[[h]]\n",
    "                predictall = predict_dataall.iloc[[h]]\n",
    "                fit_datah = fit_data.loc[0:len(fit_data)-3+h] #tau = 3 (forecast horizon)\n",
    "                \n",
    "                #fit the bayesian model to get de CPTs\n",
    "                model.fit(fit_datah)\n",
    "                model.get_cpds(node = target_variable)\n",
    "\n",
    "                #drop all variable in time window T+1 (unknown values - future states)\n",
    "                for c in predict_data.columns:\n",
    "                    if '-1' not in c:\n",
    "                        predict_data[c] = predictall[c+str('-1')]\n",
    "                del predict_data[target_variable]\n",
    "                \n",
    "                #solve limitation of unknown level \n",
    "                for col in predict_data.columns:\n",
    "                    predict_data[col][predict_data[col]>=len(set(fit_datah[col]))] = len(set(fit_datah[col]))-1\n",
    "                y_pred = model.predict(predict_data)\n",
    "                y_pred[target_variable] = y_pred[target_variable].replace(np.arange(0,len(levels[target_variable])),levels[target_variable])\n",
    "                for v in y_pred[target_variable]:\n",
    "                    aux_fore.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "                fit_data = fit_data.append(predict_data_day.loc[h-3:h-3]).reset_index(drop = True) \n",
    "            forecast_aux['Date'] = forecast_date\n",
    "            forecast_aux['Hour'] = forecast_hour\n",
    "            forecast_aux['Emissions Forecast'] = smooth(aux_fore,5)\n",
    "            real_value = real_values(pais, i)\n",
    "            forecast_aux[target_variable] = real_value[target_variable]\n",
    "            forecast_values = forecast_values.append(forecast_aux)\n",
    "            tf_inf = time.time()\n",
    "            timeinference.append(tf_inf-ti_inf)\n",
    "        k = k+1\n",
    "    #save the results on postgres\n",
    "    df_edges = pd.DataFrame()\n",
    "    df_edges['edges'] = edges_possibilities\n",
    "    df_edges['frequencia'] = edges_frequency\n",
    "    df_edges['total days'] = k-1\n",
    "    df_time_model = pd.DataFrame()\n",
    "    df_time_model['tempo'] = timemodel\n",
    "    df_time_inference = pd.DataFrame()\n",
    "    df_time_inference['tempo'] = timeinference\n",
    "    \n",
    "    #df_edges.to_sql(name='edges_frequency_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    #df_time_model.to_sql(name='time_model_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    #df_time_inference.to_sql(name='time_inference_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    #forecast_values.to_sql(name='forecast_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43cf52f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-15T01:43:05.757337Z",
     "start_time": "2022-11-15T01:09:18.659690Z"
    }
   },
   "outputs": [],
   "source": [
    "#initialize auxiliary variables\n",
    "pais = 'Alemanha'\n",
    "k=1 #total days used\n",
    "target_variable = 'Emission'\n",
    "edges_possibilities = []\n",
    "edges_frequency = []\n",
    "timemodel = []\n",
    "timeinference = []\n",
    "forecast_values = pd.DataFrame()\n",
    "\n",
    "#read all available dates\n",
    "dates = get_all_dates(pais)\n",
    "\n",
    "#begin the forecast experiment\n",
    "i = dates[6]\n",
    "#for i in tqdm(dates):\n",
    "#dataset to save de forecast values\n",
    "forecast_aux = pd.DataFrame()\n",
    "forecast_date = []\n",
    "forecast_hour = []\n",
    "forecast_v = []\n",
    "#dataset to learn the model\n",
    "data_learn = get_dataset(pais,i, i+timedelta(days = 0))\n",
    "#structural learning with the dataset of day i\n",
    "data_learn.drop(['Date','Hour'], axis = 1, inplace = True)\n",
    "ti = time.time()\n",
    "best_model = DM.EdgesModel(data_learn, FullOpt)[0]\n",
    "\n",
    "#get the markov blanket\n",
    "best_model = blanket(best_model, target_variable)\n",
    "\n",
    "#update the edges frequencies\n",
    "edges_possibilities, edges_frequency = update_edges_frequencies(best_model, edges_possibilities, edges_frequency)\n",
    "\n",
    "#update threshold and select the edges\n",
    "edges = update_threshold_select_edges(k, edges_possibilities, edges_frequency)\n",
    "if len(edges)==0:\n",
    "    edges.append(('Emission-1','Emission')) \n",
    "\n",
    "tf = time.time()\n",
    "timemodel.append(tf-ti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aea133c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-15T01:53:02.938706Z",
     "start_time": "2022-11-15T01:52:23.123183Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataset to save de forecast values \n",
    "#initialize auxiliary variables\n",
    "pais = 'Alemanha'\n",
    "k=1 #total days used\n",
    "target_variable = 'Emission'\n",
    "edges_possibilities = []\n",
    "edges_frequency = []\n",
    "timemodel = []\n",
    "timeinference = []\n",
    "forecast_values = pd.DataFrame()\n",
    "forecast_aux = pd.DataFrame()\n",
    "forecast_date = []\n",
    "forecast_hour = []\n",
    "forecast_v = []\n",
    "#forecast initial in day 8 (fit from 01 until 07)\n",
    "if i >= dates[6] and i+timedelta(days = 1) in dates:\n",
    "    #bins\n",
    "    bins = bins_values(pais)\n",
    "\n",
    "    #fit dataset (last 7 days)\n",
    "    fit_data = get_dataset(pais,i-timedelta(days = 2), i)\n",
    "    fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 2), i)\n",
    "\n",
    "    #predict data of the entire day\n",
    "    predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "    predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "\n",
    "    #detects independent variables\n",
    "    independentes=[]\n",
    "    for col in fit_data.columns:\n",
    "        if col not in list(map(lambda x: x[0],edges))+list(map(lambda x: x[1],edges)):\n",
    "            independentes.append(col)\n",
    "\n",
    "    #drop independent columns\n",
    "    fit_data.drop(independentes, axis=1, inplace = True)\n",
    "    predict_data_day.drop(independentes, axis=1, inplace = True)\n",
    "\n",
    "    #transform data in levels (limitation of PGMPY)\n",
    "    levels = {}\n",
    "    aux = fit_dataall.copy()\n",
    "    aux = aux.append(predict_dataall)\n",
    "    for var in aux.columns:\n",
    "        levels[var] = set(aux[var])\n",
    "        fit_dataall[var] = fit_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "        predict_dataall[var] = predict_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "        if var in fit_data.columns:\n",
    "            fit_data[var] = fit_data[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            predict_data_day[var] = predict_data_day[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "    predict_data_day = predict_data_day.astype(int)\n",
    "\n",
    "    #Using the edges, get the bayesian model object\n",
    "    model = BayesianModel(edges)\n",
    "\n",
    "    #aux\n",
    "    aux_fore = []\n",
    "\n",
    "    #predict each point of day i+1\n",
    "    ti_inf = time.time()\n",
    "    for h in range(len(predict_data_day)):\n",
    "        forecast_date.append(i+timedelta(days = 1))\n",
    "        forecast_hour.append(h)\n",
    "        predict_data = predict_data_day.iloc[[h]]\n",
    "        predictall = predict_dataall.iloc[[h]]\n",
    "        fit_datah = fit_data.loc[0:len(fit_data)-3+h] #tau = 3 (forecast horizon)\n",
    "        \n",
    "        smote = RandomOverSampler(random_state = 42)\n",
    "        y = fit_datah[target_variable]\n",
    "        X = fit_datah.copy()\n",
    "        del X[target_variable]\n",
    "        Xc, yc = smote.fit_resample(X,y)\n",
    "        fit_datah = Xc\n",
    "        fit_datah[target_variable] = yc\n",
    "        \n",
    "        #fit the bayesian model to get de CPTs\n",
    "        model.fit(fit_datah)\n",
    "        model.get_cpds(node = target_variable)\n",
    "\n",
    "        #drop all variable in time window T+1 (unknown values - future states)\n",
    "        for c in predict_data.columns:\n",
    "            if '-1' not in c:\n",
    "                predict_data[c] = predictall[c+str('-1')]\n",
    "        del predict_data[target_variable]\n",
    "\n",
    "        #solve limitation of unknown level \n",
    "        for col in predict_data.columns:\n",
    "            predict_data[col][predict_data[col]>=len(set(fit_datah[col]))] = len(set(fit_datah[col]))-1\n",
    "        y_pred = model.predict(predict_data)\n",
    "        y_pred[target_variable] = y_pred[target_variable].replace(np.arange(0,len(levels[target_variable])),levels[target_variable])\n",
    "        for v in y_pred[target_variable]:\n",
    "            aux_fore.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "        fit_data = fit_data.append(predict_data_day.loc[h-3:h-3]).reset_index(drop = True) \n",
    "    forecast_aux['Date'] = forecast_date\n",
    "    forecast_aux['Hour'] = forecast_hour\n",
    "    forecast_aux['Emissions Forecast'] = smooth(aux_fore,5)\n",
    "    real_value = real_values(pais, i)\n",
    "    forecast_aux[target_variable] = real_value[target_variable]\n",
    "    forecast_values = forecast_values.append(forecast_aux)\n",
    "    tf_inf = time.time()\n",
    "    timeinference.append(tf_inf-ti_inf)\n",
    "k = k+1\n",
    "#save the results on postgres\n",
    "df_edges = pd.DataFrame()\n",
    "df_edges['edges'] = edges_possibilities\n",
    "df_edges['frequencia'] = edges_frequency\n",
    "df_edges['total days'] = k-1\n",
    "df_time_model = pd.DataFrame()\n",
    "df_time_model['tempo'] = timemodel\n",
    "df_time_inference = pd.DataFrame()\n",
    "df_time_inference['tempo'] = timeinference\n",
    "\n",
    "#df_edges.to_sql(name='edges_frequency_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "#df_time_model.to_sql(name='time_model_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "#df_time_inference.to_sql(name='time_inference_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "#forecast_values.to_sql(name='forecast_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4595b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-15T01:53:02.947683Z",
     "start_time": "2022-11-15T01:53:02.939704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.973242275082256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f532450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-15T01:50:57.283455Z",
     "start_time": "2022-11-15T01:50:57.268494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Emissions Forecast</th>\n",
       "      <th>Emission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>204.480805</td>\n",
       "      <td>173.220049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>1</td>\n",
       "      <td>158.221753</td>\n",
       "      <td>167.995697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>2</td>\n",
       "      <td>193.755023</td>\n",
       "      <td>168.596713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>3</td>\n",
       "      <td>185.710686</td>\n",
       "      <td>168.655569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>4</td>\n",
       "      <td>177.666349</td>\n",
       "      <td>171.490171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>5</td>\n",
       "      <td>172.303458</td>\n",
       "      <td>181.256012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>6</td>\n",
       "      <td>166.940566</td>\n",
       "      <td>194.013618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>7</td>\n",
       "      <td>164.259121</td>\n",
       "      <td>207.777762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>8</td>\n",
       "      <td>164.259121</td>\n",
       "      <td>216.459322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>9</td>\n",
       "      <td>164.259121</td>\n",
       "      <td>209.884366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>10</td>\n",
       "      <td>174.984903</td>\n",
       "      <td>202.180287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>11</td>\n",
       "      <td>174.984903</td>\n",
       "      <td>198.266349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>12</td>\n",
       "      <td>183.029240</td>\n",
       "      <td>195.495696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>13</td>\n",
       "      <td>191.073577</td>\n",
       "      <td>194.756052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>14</td>\n",
       "      <td>191.073577</td>\n",
       "      <td>197.050331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>15</td>\n",
       "      <td>188.392132</td>\n",
       "      <td>201.191389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>16</td>\n",
       "      <td>188.392132</td>\n",
       "      <td>201.160241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>17</td>\n",
       "      <td>188.392132</td>\n",
       "      <td>201.038778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>18</td>\n",
       "      <td>188.392132</td>\n",
       "      <td>203.539358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>19</td>\n",
       "      <td>196.436468</td>\n",
       "      <td>197.487872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>20</td>\n",
       "      <td>188.392132</td>\n",
       "      <td>197.773609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>21</td>\n",
       "      <td>196.436468</td>\n",
       "      <td>190.774296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>22</td>\n",
       "      <td>155.540307</td>\n",
       "      <td>187.014120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>23</td>\n",
       "      <td>204.480805</td>\n",
       "      <td>176.655694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Hour  Emissions Forecast    Emission\n",
       "0   2019-01-08     0          204.480805  173.220049\n",
       "1   2019-01-08     1          158.221753  167.995697\n",
       "2   2019-01-08     2          193.755023  168.596713\n",
       "3   2019-01-08     3          185.710686  168.655569\n",
       "4   2019-01-08     4          177.666349  171.490171\n",
       "5   2019-01-08     5          172.303458  181.256012\n",
       "6   2019-01-08     6          166.940566  194.013618\n",
       "7   2019-01-08     7          164.259121  207.777762\n",
       "8   2019-01-08     8          164.259121  216.459322\n",
       "9   2019-01-08     9          164.259121  209.884366\n",
       "10  2019-01-08    10          174.984903  202.180287\n",
       "11  2019-01-08    11          174.984903  198.266349\n",
       "12  2019-01-08    12          183.029240  195.495696\n",
       "13  2019-01-08    13          191.073577  194.756052\n",
       "14  2019-01-08    14          191.073577  197.050331\n",
       "15  2019-01-08    15          188.392132  201.191389\n",
       "16  2019-01-08    16          188.392132  201.160241\n",
       "17  2019-01-08    17          188.392132  201.038778\n",
       "18  2019-01-08    18          188.392132  203.539358\n",
       "19  2019-01-08    19          196.436468  197.487872\n",
       "20  2019-01-08    20          188.392132  197.773609\n",
       "21  2019-01-08    21          196.436468  190.774296\n",
       "22  2019-01-08    22          155.540307  187.014120\n",
       "23  2019-01-08    23          204.480805  176.655694"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d712ca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-15T01:04:22.536384Z",
     "start_time": "2022-11-15T01:04:22.531397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Solar', 'W. Onshore-1'),\n",
       " ('Solar', 'Emission'),\n",
       " ('Emission', 'W. Onshore-1'),\n",
       " ('W. Offshore-1', 'Hard coal-1'),\n",
       " ('Emission', 'W. Onshore'),\n",
       " ('Fossil Gas', 'Emission'),\n",
       " ('Emission', 'Hard coal-1'),\n",
       " ('Fossil Gas-1', 'Hard coal-1')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5d3d009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T22:14:08.357919Z",
     "start_time": "2022-11-14T22:14:08.351934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Emission', 'W. Onshore-1'),\n",
       " ('Emission', 'W. Onshore'),\n",
       " ('Emission', 'Hard coal-1'),\n",
       " ('Fossil Gas', 'Emission'),\n",
       " ('Solar', 'Emission'),\n",
       " ('Fossil Gas-1', 'Hard coal-1'),\n",
       " ('Solar', 'W. Onshore-1'),\n",
       " ('W. Offshore-1', 'Hard coal-1')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de812ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T17:52:43.732971Z",
     "start_time": "2022-11-14T17:52:43.724993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.973242275082256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04811e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
