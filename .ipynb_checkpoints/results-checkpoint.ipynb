{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dfa39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import psycopg2 as pg\n",
    "import sqlalchemy as sq\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "logging.disable()\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def errorf (real,forecast):\n",
    "    error=[]\n",
    "    for i in range(len(real)):\n",
    "        error.append(real[i]-forecast[i])\n",
    "    return error\n",
    "\n",
    "def open_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE\n",
    "    '''\n",
    "    conn = pg.connect(dbname='postgres', user = 'postgres', password = 123, host = 'localhost')\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "defeb8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "País: Alemanha\n",
      "---\n",
      "EDBN - NRMSE 3.921667809619797 1.1186713068907033\n",
      "EDBN - MAE 34.829463117857195 12.952730961006202\n",
      "EDBN - MedAE 30.20179908312799 13.345021282202667\n",
      " \n",
      "DBN - NRMSE 4.941210764767227 1.897737997524624\n",
      "DBN - MAE 43.328653914445226 19.300549836061922\n",
      "DBN - MedAE 36.886084005559944 18.931622059532668\n",
      " \n",
      "ANN - NRMSE 4.199115007201758 1.0638331324949797\n",
      "ANN - MAE 37.019746629041705 13.597101167881345\n",
      "ANN - MedAE 31.44829347753135 13.796324358010821\n",
      " \n",
      "XGBOOST - NRMSE 4.422396098563973 1.43328816666743\n",
      "XGBOOST - MAE 37.90086912386942 14.067477037503611\n",
      "XGBOOST - MedAE 31.553329989815055 14.33496512658341\n",
      " \n",
      "País: Belgica\n",
      "---\n",
      "EDBN - NRMSE 2.539553702909607 0.7204637717511818\n",
      "EDBN - MAE 14.287299570291625 5.55638915962488\n",
      "EDBN - MedAE 12.44462801372467 5.5333677891379285\n",
      " \n",
      "DBN - NRMSE 4.258262018282658 2.6431835789225038\n",
      "DBN - MAE 25.918072343827582 18.019166798145662\n",
      "DBN - MedAE 25.105216603372416 19.662609744933537\n",
      " \n",
      "ANN - NRMSE 2.498252943385948 0.6189486487323352\n",
      "ANN - MAE 13.801400565458847 4.977005768130244\n",
      "ANN - MedAE 11.641973975170078 4.977862422720778\n",
      " \n",
      "XGBOOST - NRMSE 2.600538333814819 0.648794613406016\n",
      "XGBOOST - MAE 14.255929425086208 4.9613923831769915\n",
      "XGBOOST - MedAE 11.9913040248783 4.984630321402983\n",
      " \n",
      "País: Espanha\n",
      "---\n",
      "EDBN - NRMSE 2.412208141858258 0.7454758378362003\n",
      "EDBN - MAE 13.027818506591906 5.4838161584934575\n",
      "EDBN - MedAE 11.42588980385914 5.453580244265924\n",
      " \n",
      "DBN - NRMSE 3.261955913034255 1.9272015658052621\n",
      "DBN - MAE 16.929866792188186 10.239732119412528\n",
      "DBN - MedAE 14.049116466784069 9.09827799438466\n",
      " \n",
      "ANN - NRMSE 2.8239239850096767 0.7968328291871566\n",
      "ANN - MAE 14.97304743335307 5.552870353283794\n",
      "ANN - MedAE 12.929649966383677 5.694196317971665\n",
      " \n",
      "XGBOOST - NRMSE 2.765959169518471 0.8538084236584815\n",
      "XGBOOST - MAE 14.41762367298401 5.4802385425636695\n",
      "XGBOOST - MedAE 12.106892963841556 5.262993939055408\n",
      " \n",
      "País: Portugal\n",
      "---\n",
      "EDBN - NRMSE 3.830604776224206 1.396608272636586\n",
      "EDBN - MAE 30.90450116769945 15.864700379699295\n",
      "EDBN - MedAE 26.825729996669676 15.742904663101765\n",
      " \n",
      "DBN - NRMSE 5.710682753090539 3.6542737293115213\n",
      "DBN - MAE 46.91575537300879 29.364193036121836\n",
      "DBN - MedAE 43.82195844306684 31.383552760328588\n",
      " \n",
      "ANN - NRMSE 3.755518178239163 1.1479731177261516\n",
      "ANN - MAE 30.218785772457718 14.226397258620423\n",
      "ANN - MedAE 25.942126317428624 14.297173295545743\n",
      " \n",
      "XGBOOST - NRMSE 4.088032604734439 1.449945221745372\n",
      "XGBOOST - MAE 31.880973678491447 15.220954452905525\n",
      "XGBOOST - MedAE 26.60739167943418 14.995899589584424\n",
      " \n"
     ]
    }
   ],
   "source": [
    "countries = ['Alemanha', 'Belgica','Espanha', 'Portugal']\n",
    "conn = open_connection()\n",
    "for c in countries:\n",
    "    nrmse_edbn = []\n",
    "    mae_edbn = []\n",
    "    medae_edbn = []\n",
    "    \n",
    "    nrmse_dbn = []\n",
    "    mae_dbn = []\n",
    "    medae_dbn = []\n",
    "    \n",
    "    nrmse_ann = []\n",
    "    mae_ann = []\n",
    "    medae_ann = []\n",
    "    \n",
    "    nrmse_xgboost = []\n",
    "    mae_xgboost = []\n",
    "    medae_xgboost = []\n",
    "    \n",
    "    q = f'''select * from results.\"forecast_final_{c.lower()}\"'''\n",
    "    edbn = pd.read_sql(q,conn)\n",
    "    \n",
    "    q = f'''select * from results.\"forecast_final_{c.lower()}dbn_onestep\"'''\n",
    "    dbn = pd.read_sql(q,conn)\n",
    "    \n",
    "    q = f'''select * from results.\"forecast_{c}ann\"'''\n",
    "    ann = pd.read_sql(q,conn)\n",
    "    \n",
    "    q = f'''select * from results.\"forecast_{c}xgboost\"'''\n",
    "    xgboost = pd.read_sql(q,conn)\n",
    "    \n",
    "    for d in edbn['Date'].unique():\n",
    "        #edbn\n",
    "        nrmse_edbn.append(np.sqrt((mean_squared_error(edbn['Emission'][edbn['Date']==d],edbn['Emissions Forecast'][edbn['Date']==d]))/(max(edbn['Emission'][edbn['Date']==d])-min(edbn['Emission'][edbn['Date']==d]))))\n",
    "        mae_edbn.append(mean_absolute_error(edbn['Emission'][edbn['Date']==d],edbn['Emissions Forecast'][edbn['Date']==d]))\n",
    "        medae_edbn.append(median_absolute_error(edbn['Emission'][edbn['Date']==d],edbn['Emissions Forecast'][edbn['Date']==d]))\n",
    "        \n",
    "        #dbn\n",
    "        nrmse_dbn.append(np.sqrt((mean_squared_error(dbn['Emission'][dbn['Date']==d],dbn['Emissions Forecast'][dbn['Date']==d]))/(max(dbn['Emission'][dbn['Date']==d])-min(dbn['Emission'][dbn['Date']==d]))))\n",
    "        mae_dbn.append(mean_absolute_error(dbn['Emission'][dbn['Date']==d],dbn['Emissions Forecast'][dbn['Date']==d]))\n",
    "        medae_dbn.append(median_absolute_error(dbn['Emission'][dbn['Date']==d],dbn['Emissions Forecast'][dbn['Date']==d]))\n",
    "        \n",
    "        #ANN\n",
    "        nrmse_ann.append(np.sqrt((mean_squared_error(ann['Emission'][ann['Date']==d],ann['Emissions Forecast'][ann['Date']==d]))/(max(ann['Emission'][ann['Date']==d])-min(ann['Emission'][ann['Date']==d]))))\n",
    "        mae_ann.append(mean_absolute_error(ann['Emission'][ann['Date']==d],ann['Emissions Forecast'][ann['Date']==d]))\n",
    "        medae_ann.append(median_absolute_error(ann['Emission'][ann['Date']==d],ann['Emissions Forecast'][ann['Date']==d]))\n",
    "\n",
    "        #xgboot\n",
    "        nrmse_xgboost.append(np.sqrt((mean_squared_error(xgboost['Emission'][xgboost['Date']==d],xgboost['Emissions Forecast'][xgboost['Date']==d]))/(max(xgboost['Emission'][xgboost['Date']==d])-min(xgboost['Emission'][xgboost['Date']==d]))))\n",
    "        mae_xgboost.append(mean_absolute_error(xgboost['Emission'][xgboost['Date']==d],xgboost['Emissions Forecast'][xgboost['Date']==d]))\n",
    "        medae_xgboost.append(median_absolute_error(xgboost['Emission'][xgboost['Date']==d],xgboost['Emissions Forecast'][xgboost['Date']==d]))\n",
    "\n",
    "    print('País:', c)\n",
    "    print('---')\n",
    "    print('EDBN - NRMSE',np.mean(nrmse_edbn),np.std(nrmse_edbn))\n",
    "    print('EDBN - MAE',np.mean(mae_edbn),np.std(mae_edbn))\n",
    "    print('EDBN - MedAE',np.mean(medae_edbn),np.std(medae_edbn))\n",
    "    print(' ')\n",
    "    print('DBN - NRMSE',np.mean(nrmse_dbn),np.std(nrmse_dbn))\n",
    "    print('DBN - MAE',np.mean(mae_dbn),np.std(mae_dbn))\n",
    "    print('DBN - MedAE',np.mean(medae_dbn),np.std(medae_dbn))\n",
    "    print(' ')\n",
    "    print('ANN - NRMSE',np.mean(nrmse_ann),np.std(nrmse_ann))\n",
    "    print('ANN - MAE',np.mean(mae_ann),np.std(mae_ann))\n",
    "    print('ANN - MedAE',np.mean(medae_ann),np.std(medae_ann))\n",
    "    print(' ')\n",
    "    print('XGBOOST - NRMSE',np.mean(nrmse_xgboost),np.std(nrmse_xgboost))\n",
    "    print('XGBOOST - MAE',np.mean(mae_xgboost),np.std(mae_xgboost))\n",
    "    print('XGBOOST - MedAE',np.mean(medae_xgboost),np.std(medae_xgboost))\n",
    "    print(' ')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2d657db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "País: Alemanha\n",
      "---\n",
      "EDBN - NRMSE 3.921667809619797 1.1186713068907033\n",
      "EDBN - MAE 34.829463117857195 12.952730961006202\n",
      "EDBN - MedAE 30.20179908312799 13.345021282202667\n",
      " \n",
      "DBN - NRMSE 4.941210764767227 1.897737997524624\n",
      "DBN - MAE 43.328653914445226 19.300549836061922\n",
      "DBN - MedAE 36.886084005559944 18.931622059532668\n",
      " \n",
      "ANN - NRMSE 4.199115007201758 1.0638331324949797\n",
      "ANN - MAE 37.019746629041705 13.597101167881345\n",
      "ANN - MedAE 31.44829347753135 13.796324358010821\n",
      " \n",
      "XGBOOST - NRMSE 4.422396098563973 1.43328816666743\n",
      "XGBOOST - MAE 37.90086912386942 14.067477037503611\n",
      "XGBOOST - MedAE 31.553329989815055 14.33496512658341\n",
      " \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f216d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NRMSE: \",np.sqrt((mean_squared_error(aux_fore['Emission'],aux_fore['Emissions Forecast']))/(max(aux_fore['Emission'])-min(aux_fore['Emission']))))\n",
    "print(\"MAE: \", mean_absolute_error(aux_fore['Emission'],aux_fore['Emissions Forecast']))\n",
    "print(\"MedAE: \", median_absolute_error(aux_fore['Emission'],aux_fore['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538c2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.43147426708343\n"
     ]
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_final_portugal\" fa '''\n",
    "conn = open_connection()\n",
    "forecast_values = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "print(median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d84e35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.20216672755499"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_Portugalann\" fa '''\n",
    "conn = open_connection()\n",
    "datasetln = pd.read_sql(q,conn)\n",
    "datasetln \n",
    "conn.close()\n",
    "median_absolute_error(datasetln['Emission'],datasetln['Emissions Forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49863e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.977954481730833"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_Espanhaxgboost\" fa '''\n",
    "conn = open_connection()\n",
    "datasetln = pd.read_sql(q,conn)\n",
    "datasetln \n",
    "conn.close()\n",
    "median_absolute_error(datasetln['Emission'],datasetln['Emissions Forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47e0165c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.104894437510637"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_Portugalxgboost\" fa '''\n",
    "conn = open_connection()\n",
    "datasetln = pd.read_sql(q,conn)\n",
    "datasetln \n",
    "conn.close()\n",
    "median_absolute_error(datasetln['Emission'],datasetln['Emissions Forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b043b9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.071545718202437\n"
     ]
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_final_espanha\" fa '''\n",
    "conn = open_connection()\n",
    "forecast_values = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "print(median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50b1212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.969500794151479\n"
     ]
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_final_belgica\" fa '''\n",
    "conn = open_connection()\n",
    "forecast_values = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "print(median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b956062f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.76468869788097\n"
     ]
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_final_alemanha\" fa '''\n",
    "conn = open_connection()\n",
    "forecast_values = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "print(median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c1c875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.358640315443154\n"
     ]
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_final2_alemanha\" fa '''\n",
    "conn = open_connection()\n",
    "forecast_values = pd.read_sql(q,conn)\n",
    "conn.close()\n",
    "print(median_absolute_error(forecast_values['Emission'],forecast_values['Emissions Forecast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33893a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.358425700451278"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '''select * \n",
    "from results.\"forecast_Alemanhaann\" fa '''\n",
    "conn = open_connection()\n",
    "datasetln = pd.read_sql(q,conn)\n",
    "datasetln \n",
    "conn.close()\n",
    "median_absolute_error(datasetln['Emission'],datasetln['Emissions Forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28702fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7ab899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import HillClimbSearch\n",
    "from datetime import datetime, timedelta\n",
    "from pgmpy.inference import VariableElimination\n",
    "import random\n",
    "import warnings\n",
    "import sys \n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import psycopg2 as pg\n",
    "import sqlalchemy as sq\n",
    "import networkx as nx\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.estimators import BDeuScore \n",
    "from pgmpy.estimators import K2Score\n",
    "from pgmpy.estimators import BicScore\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "import logging\n",
    "logging.disable()\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    vi = y[0]\n",
    "    vf = y[len(y)-1]\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    y_smooth[0] = vi\n",
    "    y_smooth[len(y_smooth)-1] = vf\n",
    "    return y_smooth\n",
    "\n",
    "def errorf (real,forecast):\n",
    "    error=[]\n",
    "    for i in range(len(real)):\n",
    "        error.append(real[i]-forecast[i])\n",
    "    return error\n",
    "\n",
    "def open_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE\n",
    "    '''\n",
    "    conn = pg.connect(dbname='postgres', user = 'postgres', password = 123, host = 'localhost')\n",
    "    return conn\n",
    "def get_connection():\n",
    "    '''\n",
    "    FUNCTION TO CONNECT TO THE POSTGRESQL DATABASE AND RETURN THE SQLACHEMY ENGINE OBJECT\n",
    "    -----------\n",
    "    output: object\n",
    "        SQLACHEMY ENGINE OBJECT - POSTGRESQL DATABASE CONNECTION\n",
    "    '''\n",
    "    user = 'postgres'\n",
    "    password = 123\n",
    "    host = 'localhost'\n",
    "    port = 5432\n",
    "    database = 'postgres'\n",
    "    return sq.create_engine(url=\"postgresql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database))\n",
    "\n",
    "def blanket(model,variable):\n",
    "    '''\n",
    "    Function to extract the Markov Blanket of the target variable (reduces the structure)\n",
    "    -----------\n",
    "    input:\n",
    "    model: list\n",
    "        list of edges\n",
    "        \n",
    "    variable: str\n",
    "        name of the target variable\n",
    "        \n",
    "    output:\n",
    "    blanket: list\n",
    "        list of edges \n",
    "    '''\n",
    "    blanket=[]\n",
    "    sons=[]\n",
    "    for i in model:\n",
    "        if i[0]==variable or i[1]==variable:\n",
    "            blanket.append(i)\n",
    "        if i[0]==variable:\n",
    "            sons.append(i[1])\n",
    "    for i in model:\n",
    "        if i[1] in sons and i[0]!=variable:\n",
    "            blanket.append(i)\n",
    "    return blanket\n",
    "\n",
    "def verifica_remove_ciclos(edges):\n",
    "    '''\n",
    "    Function to verify if the edges is a DAG and to try remove cycles\n",
    "    -----------\n",
    "    input:\n",
    "    edges: list\n",
    "        list of edges\n",
    "                \n",
    "    output:\n",
    "    blanket: list\n",
    "        list of edges \n",
    "    '''\n",
    "    edgesdag = edges #recebe o próprio modelo\n",
    "    #Verifica se tem ciclos e tenta remover invertendo uma aresta\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:  # (3) flip single edge\n",
    "            edges2 = edgesdag.copy()\n",
    "            edges2.extend([i[::-1]])\n",
    "            new_edges = edges2.copy()\n",
    "            new_edges.remove(i)\n",
    "            if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                edgesdag = new_edges.copy()\n",
    "                break\n",
    "    #Verifica se tem ciclos e tenta remover invertendo duas arestas\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:\n",
    "            for j in edgesdag:# (3) flip two edges\n",
    "                if i != j:\n",
    "                    edges2 = edgesdag.copy()\n",
    "                    edges2.extend([i[::-1]])\n",
    "                    edges2.extend([j[::-1]])\n",
    "                    new_edges = edges2.copy()\n",
    "                    new_edges.remove(i)\n",
    "                    new_edges.remove(j)\n",
    "                    if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                        edgesdag = new_edges.copy()\n",
    "                        breaker = True\n",
    "                        break\n",
    "            if breaker:\n",
    "                break\n",
    "    #Verifica se tem ciclos e tenta remover invertendo uma aresta e excluindo uma aresta\n",
    "    if ~nx.is_directed_acyclic_graph(nx.DiGraph(edgesdag[:])):\n",
    "        for i in edgesdag:\n",
    "            for j in edgesdag:# (3) flip two edges\n",
    "                if i != j:\n",
    "                    edges2 = edgesdag.copy()\n",
    "                    edges2.extend([i[::-1]])\n",
    "                    new_edges = edges2.copy()\n",
    "                    new_edges.remove(i)\n",
    "                    new_edges.remove(j)\n",
    "                    if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges[:])):\n",
    "                        edgesdag = new_edges.copy()\n",
    "                        breaker = True\n",
    "                        break\n",
    "            if breaker:\n",
    "                break\n",
    "    return edgesdag\n",
    "def get_all_dates(pais):\n",
    "    q = '''select distinct cast(\"Date\" as DATE) as datas from pre_processed_data.dbn_features_selected_{pais} order by datas'''.format(pais=pais)\n",
    "    conn = open_connection()\n",
    "    date = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    datas = date['datas'].tolist()\n",
    "    return datas\n",
    "\n",
    "def get_dataset(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_features_selected_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "def get_dataset_allfeatures(pais,date_ini, date_fin):\n",
    "    q = '''select * \n",
    "    from pre_processed_data.dbn_{pais} where \"Date\" between '{date_ini}' and '{date_fin}' '''.format(pais=pais,date_ini=date_ini,date_fin=date_fin)\n",
    "    conn = open_connection()\n",
    "    dataset = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return dataset\n",
    "\n",
    "def update_edges_frequencies(best_model, edges_possibilities, edges_frequency):\n",
    "    if not edges_possibilities:\n",
    "        edges_possibilities = best_model\n",
    "        for p in range(len(edges_possibilities)):\n",
    "            edges_frequency.append(1)\n",
    "    else:\n",
    "        for v in range(len(best_model)):\n",
    "            if best_model[v] not in edges_possibilities:\n",
    "                edges_possibilities.append(best_model[v])\n",
    "                edges_frequency.append(1)\n",
    "            else:\n",
    "                for f in range(len(edges_possibilities)):\n",
    "                    if best_model[v] == edges_possibilities[f]:\n",
    "                        edges_frequency[f]=edges_frequency[f]+1\n",
    "    return edges_possibilities, edges_frequency\n",
    "\n",
    "def update_threshold_select_edges(k, edges_possibilities, edges_frequency):\n",
    "    fth = 1/3+np.sqrt(2/k)\n",
    "    if fth>0.4:\n",
    "        fth=0.4\n",
    "    edges_frequency_v=[edges_frequency[i]/k for i in range(len(edges_frequency))]\n",
    "    edges=[]\n",
    "    for i in range(len(edges_possibilities)):\n",
    "        if edges_frequency_v[i]>=fth and edges_possibilities[i] not in edges:\n",
    "            if edges_possibilities[i][::-1] not in edges_possibilities:\n",
    "                edges.append(edges_possibilities[i])\n",
    "            else: \n",
    "                if edges_frequency_v[i] > edges_frequency_v[edges_possibilities.index(edges_possibilities[i][::-1])]:\n",
    "                    edges.append(edges_possibilities[i])\n",
    "                else: \n",
    "                    edges.append(edges_possibilities[i][::-1])\n",
    "        elif edges_frequency_v[i]<fth:\n",
    "            for j in range(len(edges_possibilities)):\n",
    "                if edges_possibilities[i]==edges_possibilities[j][::-1]:\n",
    "                    if edges_frequency_v[i]+edges_frequency_v[j]>=fth:\n",
    "                        if edges_frequency_v[i]>edges_frequency_v[j]:\n",
    "                            edges.append(edges_possibilities[i])\n",
    "                        if edges_frequency_v[i]<edges_frequency_v[j]:\n",
    "                            edges.append(edges_possibilities[j])\n",
    "                        if edges_frequency_v[i]==edges_frequency_v[j]:\n",
    "                            auxci=0\n",
    "                            auxcj=0\n",
    "                            for s in range(len(edges)):\n",
    "                                if edges[s]==edges_possibilities[i]:\n",
    "                                    auxci=auxci+1\n",
    "                                if edges[s]==edges_possibilities[j]:\n",
    "                                    auxcj=auxcj+1\n",
    "                            if auxci>0:\n",
    "                                edges.append(edges_possibilities[i])\n",
    "                            elif auxcj>0:\n",
    "                                edges.append(edges_possibilities[j])\n",
    "                            else: \n",
    "                                import random\n",
    "                                edges.append(random.choice([edges_possibilities[i],edges_possibilities[j]]))\n",
    "    edges = list(set(edges))\n",
    "    return edges\n",
    "\n",
    "def bins_values(pais):\n",
    "    q = '''select \"Emission\" from pre_processed_data.bins_{pais} where \"Emission\" is not null'''.format(pais = pais)\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def real_values(pais, data):\n",
    "    q = '''select \"Emission\" from pre_processed_data.{pais} where \"Date\" = '{dataf}' '''.format(pais = pais, dataf = (data+timedelta(days = 1)).strftime(\"%Y/%m/%d\"))\n",
    "    conn = open_connection()\n",
    "    df = pd.read_sql(q,conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def main(pais,from_stop):\n",
    "    if from_stop == True:\n",
    "        target_variable = 'Emission'\n",
    "        conn = open_connection()\n",
    "        q = f'''select * from results.edges_frequency_final_{pais}'''\n",
    "        dados_arestas = pd.read_sql(q,conn)\n",
    "        edges_possibilities = dados_arestas['edges'].tolist()\n",
    "        edges_frequency = dados_arestas['frequencia'].tolist()\n",
    "        k = dados_arestas['total days'][0]\n",
    "        \n",
    "        q = f'''select * from results.forecast_final_{pais}'''\n",
    "        forecast_values = pd.read_sql(q,conn)\n",
    "        \n",
    "        q = f'''select * from results.time_model_final_{pais}'''\n",
    "        timemodel = pd.read_sql(q,conn)\n",
    "        timemodel = timemodel['tempo'].tolist()\n",
    "        \n",
    "        q = f'''select * from results.time_inference_final_{pais}'''\n",
    "        timeinference = pd.read_sql(q,conn)\n",
    "        timeinference = timeinference['tempo'].tolist()\n",
    "        conn.close()\n",
    "        \n",
    "    else: \n",
    "        #initialize auxiliary variables\n",
    "        k=1 #total days used\n",
    "        target_variable = 'Emission'\n",
    "        edges_possibilities = []\n",
    "        edges_frequency = []\n",
    "        timemodel = []\n",
    "        timeinference = []\n",
    "        forecast_values = pd.DataFrame()\n",
    "\n",
    "    #read all available dates\n",
    "    dates = get_all_dates(pais)[k-1:]\n",
    "\n",
    "    #begin the forecast experiment\n",
    "    for i in tqdm(dates):\n",
    "        #dataset to save de forecast values\n",
    "        forecast_aux = pd.DataFrame()\n",
    "        forecast_date = []\n",
    "        forecast_hour = []\n",
    "        forecast_v = []\n",
    "        #dataset to learn the model\n",
    "        data_learn = get_dataset(pais,i, i+timedelta(days = 0))\n",
    "        #structural learning with the dataset of day i\n",
    "        data_learn.drop(['Date','Hour'], axis = 1, inplace = True)\n",
    "        smote = SMOTE()\n",
    "        y = data_learn[target_variable]\n",
    "        X = data_learn.copy()\n",
    "        del X[target_variable]\n",
    "        Xc, yc = smote.fit_resample(X,y)\n",
    "        data_learn = Xc\n",
    "        data_learn[target_variable] = yc\n",
    "        \n",
    "        ti = time.time()\n",
    "        est = HillClimbSearch(data_learn)\n",
    "        best_model = est.estimate(scoring_method=BicScore(data_learn),  show_progress=False)\n",
    "        best_model = list(best_model.edges())\n",
    "        if len(best_model)==0:\n",
    "            best_model = [('Emission-1','Emission')]\n",
    "\n",
    "        #get the markov blanket\n",
    "        best_model = blanket(best_model, target_variable)\n",
    "\n",
    "        #update the edges frequencies\n",
    "        edges_possibilities, edges_frequency = update_edges_frequencies(best_model, edges_possibilities, edges_frequency)\n",
    "\n",
    "        #update threshold and select the edges\n",
    "        edges = update_threshold_select_edges(k, edges_possibilities, edges_frequency)\n",
    "        if len(edges)==0:\n",
    "            edges.append(('Emission-1','Emission')) \n",
    "\n",
    "        tf = time.time()\n",
    "        timemodel.append(tf-ti)\n",
    "        #forecast initial in day 8 (fit from 01 until 07)\n",
    "        \n",
    "        if from_stop == True:\n",
    "            inicio = dates[0]\n",
    "        else:\n",
    "            inicio = dates[6]\n",
    "        if i >= inicio and i+timedelta(days = 1) in dates:\n",
    "            #bins\n",
    "            bins = bins_values(pais)\n",
    "            \n",
    "            #fit dataset (last 3 days)\n",
    "            fit_data = get_dataset(pais,i-timedelta(days = 2), i)\n",
    "            fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 2), i)\n",
    "\n",
    "            #predict data of the entire day\n",
    "            predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "            predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "\n",
    "            #detects independent variables\n",
    "            independentes=[]\n",
    "            for col in fit_data.columns:\n",
    "                if col not in list(map(lambda x: x[0],edges))+list(map(lambda x: x[1],edges)):\n",
    "                    independentes.append(col)\n",
    "\n",
    "            #drop independent columns\n",
    "            fit_data.drop(independentes, axis=1, inplace = True)\n",
    "            predict_data_day.drop(independentes, axis=1, inplace = True)\n",
    "\n",
    "            #transform data in levels (limitation of PGMPY)\n",
    "            levels = {}\n",
    "            aux = fit_dataall.copy()\n",
    "            aux = aux.append(predict_dataall)\n",
    "            for var in aux.columns:\n",
    "                levels[var] = set(aux[var])\n",
    "                fit_dataall[var] = fit_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                predict_dataall[var] = predict_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                if var in fit_data.columns:\n",
    "                    fit_data[var] = fit_data[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                    predict_data_day[var] = predict_data_day[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            predict_data_day = predict_data_day.astype(int)\n",
    "            \n",
    "            #Using the edges, get the bayesian model object\n",
    "            model = BayesianNetwork(edges)\n",
    "\n",
    "            #aux\n",
    "            aux_fore = []\n",
    "        \n",
    "            #predict each point of day i+1\n",
    "            ti_inf = time.time()\n",
    "            for h in range(len(predict_data_day)):\n",
    "                forecast_date.append(i+timedelta(days = 1))\n",
    "                forecast_hour.append(h)\n",
    "                predict_data = predict_data_day.iloc[[h]]\n",
    "                predictall = predict_dataall.iloc[[h]]\n",
    "                fit_datah = fit_data.loc[0:len(fit_data)-3+h] #tau = 3 (forecast horizon)\n",
    "                \n",
    "                smote = SMOTE()\n",
    "                y = fit_datah[target_variable]\n",
    "                X = fit_datah.copy()\n",
    "                del X[target_variable]\n",
    "                Xc, yc = smote.fit_resample(X,y)\n",
    "                fit_datah = Xc\n",
    "                fit_datah[target_variable] = yc\n",
    "                \n",
    "                #fit the bayesian model to get de CPTs\n",
    "                model.fit(fit_datah,n_jobs = 1)\n",
    "                #model.get_cpds(node = target_variable)\n",
    "\n",
    "                #drop all variable in time window T+1 (unknown values - future states)\n",
    "                for c in predict_data.columns:\n",
    "                    if '-1' not in c:\n",
    "                        predict_data[c] = predictall[c+str('-1')]\n",
    "                del predict_data[target_variable]\n",
    "                \n",
    "                #solve limitation of unknown level \n",
    "                for col in predict_data.columns:\n",
    "                    predict_data[col][predict_data[col]>=len(set(fit_datah[col]))] = len(set(fit_datah[col]))-1\n",
    "                y_pred = model.predict(predict_data,n_jobs = 1)\n",
    "                y_pred[target_variable] = y_pred[target_variable].replace(np.arange(0,len(levels[target_variable])),levels[target_variable])\n",
    "                for v in y_pred[target_variable]:\n",
    "                    aux_fore.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "                fit_data = fit_data.append(predict_data_day.loc[h-3:h-3]).reset_index(drop = True) \n",
    "            forecast_aux['Date'] = forecast_date\n",
    "            forecast_aux['Hour'] = forecast_hour\n",
    "            forecast_aux['Emissions Forecast'] = smooth(aux_fore,3)\n",
    "            real_value = real_values(pais, i)\n",
    "            forecast_aux[target_variable] = real_value[target_variable]\n",
    "            forecast_values = forecast_values.append(forecast_aux)\n",
    "            tf_inf = time.time()\n",
    "            timeinference.append(tf_inf-ti_inf)\n",
    "        k = k+1\n",
    "    #save the results on postgres\n",
    "    df_edges = pd.DataFrame()\n",
    "    df_edges['edges'] = edges_possibilities\n",
    "    df_edges['frequencia'] = edges_frequency\n",
    "    df_edges['total days'] = k-1\n",
    "    df_time_model = pd.DataFrame()\n",
    "    df_time_model['tempo'] = timemodel\n",
    "    df_time_inference = pd.DataFrame()\n",
    "    df_time_inference['tempo'] = timeinference\n",
    "\n",
    "    df_edges.to_sql(name='edges_frequency_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    df_time_model.to_sql(name='time_model_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    df_time_inference.to_sql(name='time_inference_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "    forecast_values.to_sql(name='forecast_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de479643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1091 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 5, n_neighbors = 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m X \u001b[38;5;241m=\u001b[39m data_learn\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m X[target_variable]\n\u001b[0;32m---> 52\u001b[0m Xc, yc \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m data_learn \u001b[38;5;241m=\u001b[39m Xc\n\u001b[1;32m     54\u001b[0m data_learn[target_variable] \u001b[38;5;241m=\u001b[39m yc\n",
      "File \u001b[0;32m~/anaconda3/envs/env_teste/lib/python3.9/site-packages/imblearn/base.py:83\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m     81\u001b[0m )\n\u001b[0;32m---> 83\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     86\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     89\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m~/anaconda3/envs/env_teste/lib/python3.9/site-packages/imblearn/over_sampling/_smote/filter.py:427\u001b[0m, in \u001b[0;36mSVMSMOTE._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    425\u001b[0m n_generated_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(fractions \u001b[38;5;241m*\u001b[39m (n_samples \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcount_nonzero(danger_bool) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 427\u001b[0m     nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_k_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupport_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdanger_bool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    432\u001b[0m     X_new_1, y_new_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_samples(\n\u001b[1;32m    433\u001b[0m         _safe_indexing(support_vector, np\u001b[38;5;241m.\u001b[39mflatnonzero(danger_bool)),\n\u001b[1;32m    434\u001b[0m         y\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m         step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcount_nonzero(safety_bool) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/env_teste/lib/python3.9/site-packages/sklearn/neighbors/_base.py:749\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    747\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    750\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected n_neighbors <= n_samples, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but n_samples = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, n_neighbors = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_samples_fit, n_neighbors)\n\u001b[1;32m    752\u001b[0m     )\n\u001b[1;32m    754\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    755\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 5, n_neighbors = 6"
     ]
    }
   ],
   "source": [
    "pais = 'alemanha'\n",
    "from_stop = False\n",
    "if from_stop == True:\n",
    "    target_variable = 'Emission'\n",
    "    conn = open_connection()\n",
    "    q = f'''select * from results.edges_frequency_final_{pais}'''\n",
    "    dados_arestas = pd.read_sql(q,conn)\n",
    "    edges_possibilities = dados_arestas['edges'].tolist()\n",
    "    edges_frequency = dados_arestas['frequencia'].tolist()\n",
    "    k = dados_arestas['total days'][0]\n",
    "\n",
    "    q = f'''select * from results.forecast_final_{pais}'''\n",
    "    forecast_values = pd.read_sql(q,conn)\n",
    "\n",
    "    q = f'''select * from results.time_model_final_{pais}'''\n",
    "    timemodel = pd.read_sql(q,conn)\n",
    "    timemodel = timemodel['tempo'].tolist()\n",
    "\n",
    "    q = f'''select * from results.time_inference_final_{pais}'''\n",
    "    timeinference = pd.read_sql(q,conn)\n",
    "    timeinference = timeinference['tempo'].tolist()\n",
    "    conn.close()\n",
    "\n",
    "else: \n",
    "    #initialize auxiliary variables\n",
    "    k=1 #total days used\n",
    "    target_variable = 'Emission'\n",
    "    edges_possibilities = []\n",
    "    edges_frequency = []\n",
    "    timemodel = []\n",
    "    timeinference = []\n",
    "    forecast_values = pd.DataFrame()\n",
    "\n",
    "#read all available dates\n",
    "dates = get_all_dates(pais)[k-1:]\n",
    "\n",
    "#begin the forecast experiment\n",
    "for i in tqdm(dates):\n",
    "    #dataset to save de forecast values\n",
    "    forecast_aux = pd.DataFrame()\n",
    "    forecast_date = []\n",
    "    forecast_hour = []\n",
    "    forecast_v = []\n",
    "    #dataset to learn the model\n",
    "    data_learn = get_dataset(pais,i, i+timedelta(days = 0))\n",
    "    #structural learning with the dataset of day i\n",
    "    data_learn.drop(['Date','Hour'], axis = 1, inplace = True)\n",
    "    smote = SVMSMOTE()\n",
    "    y = data_learn[target_variable]\n",
    "    X = data_learn.copy()\n",
    "    del X[target_variable]\n",
    "    Xc, yc = smote.fit_resample(X,y)\n",
    "    data_learn = Xc\n",
    "    data_learn[target_variable] = yc\n",
    "\n",
    "    ti = time.time()\n",
    "    est = HillClimbSearch(data_learn)\n",
    "    best_model = est.estimate(scoring_method=BicScore(data_learn),  show_progress=False)\n",
    "    best_model = list(best_model.edges())\n",
    "    if len(best_model)==0:\n",
    "        best_model = [('Emission-1','Emission')]\n",
    "\n",
    "    #get the markov blanket\n",
    "    best_model = blanket(best_model, target_variable)\n",
    "\n",
    "    #update the edges frequencies\n",
    "    edges_possibilities, edges_frequency = update_edges_frequencies(best_model, edges_possibilities, edges_frequency)\n",
    "\n",
    "    #update threshold and select the edges\n",
    "    edges = update_threshold_select_edges(k, edges_possibilities, edges_frequency)\n",
    "    if len(edges)==0:\n",
    "        edges.append(('Emission-1','Emission')) \n",
    "\n",
    "    tf = time.time()\n",
    "    timemodel.append(tf-ti)\n",
    "    #forecast initial in day 8 (fit from 01 until 07)\n",
    "\n",
    "    if from_stop == True:\n",
    "        inicio = dates[0]\n",
    "    else:\n",
    "        inicio = dates[6]\n",
    "    if i >= inicio and i+timedelta(days = 1) in dates:\n",
    "        #bins\n",
    "        bins = bins_values(pais)\n",
    "\n",
    "        #fit dataset (last 3 days)\n",
    "        fit_data = get_dataset(pais,i-timedelta(days = 2), i)\n",
    "        fit_dataall = get_dataset_allfeatures(pais,i-timedelta(days = 2), i)\n",
    "\n",
    "        #predict data of the entire day\n",
    "        predict_data_day = get_dataset(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "        predict_dataall = get_dataset_allfeatures(pais,i+timedelta(days = 1), i+timedelta(days = 1))\n",
    "\n",
    "        #detects independent variables\n",
    "        independentes=[]\n",
    "        for col in fit_data.columns:\n",
    "            if col not in list(map(lambda x: x[0],edges))+list(map(lambda x: x[1],edges)):\n",
    "                independentes.append(col)\n",
    "\n",
    "        #drop independent columns\n",
    "        fit_data.drop(independentes, axis=1, inplace = True)\n",
    "        predict_data_day.drop(independentes, axis=1, inplace = True)\n",
    "\n",
    "        #transform data in levels (limitation of PGMPY)\n",
    "        levels = {}\n",
    "        aux = fit_dataall.copy()\n",
    "        aux = aux.append(predict_dataall)\n",
    "        for var in aux.columns:\n",
    "            levels[var] = set(aux[var])\n",
    "            fit_dataall[var] = fit_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            predict_dataall[var] = predict_dataall[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "            if var in fit_data.columns:\n",
    "                fit_data[var] = fit_data[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "                predict_data_day[var] = predict_data_day[var].replace(levels[var], np.arange(0,len(levels[var])))\n",
    "        predict_data_day = predict_data_day.astype(int)\n",
    "\n",
    "        #Using the edges, get the bayesian model object\n",
    "        model = BayesianNetwork(edges)\n",
    "\n",
    "        #aux\n",
    "        aux_fore = []\n",
    "\n",
    "        #predict each point of day i+1\n",
    "        ti_inf = time.time()\n",
    "        for h in range(len(predict_data_day)):\n",
    "            forecast_date.append(i+timedelta(days = 1))\n",
    "            forecast_hour.append(h)\n",
    "            predict_data = predict_data_day.iloc[[h]]\n",
    "            predictall = predict_dataall.iloc[[h]]\n",
    "            fit_datah = fit_data.loc[0:len(fit_data)-3+h] #tau = 3 (forecast horizon)\n",
    "\n",
    "            smote = SVMSMOTE()\n",
    "            y = fit_datah[target_variable]\n",
    "            X = fit_datah.copy()\n",
    "            del X[target_variable]\n",
    "            Xc, yc = smote.fit_resample(X,y)\n",
    "            fit_datah = Xc\n",
    "            fit_datah[target_variable] = yc\n",
    "\n",
    "            #fit the bayesian model to get de CPTs\n",
    "            model.fit(fit_datah,n_jobs = 1)\n",
    "            #model.get_cpds(node = target_variable)\n",
    "\n",
    "            #drop all variable in time window T+1 (unknown values - future states)\n",
    "            for c in predict_data.columns:\n",
    "                if '-1' not in c:\n",
    "                    predict_data[c] = predictall[c+str('-1')]\n",
    "            del predict_data[target_variable]\n",
    "\n",
    "            #solve limitation of unknown level \n",
    "            for col in predict_data.columns:\n",
    "                predict_data[col][predict_data[col]>=len(set(fit_datah[col]))] = len(set(fit_datah[col]))-1\n",
    "            y_pred = model.predict(predict_data,n_jobs = 1)\n",
    "            y_pred[target_variable] = y_pred[target_variable].replace(np.arange(0,len(levels[target_variable])),levels[target_variable])\n",
    "            for v in y_pred[target_variable]:\n",
    "                aux_fore.append((bins[target_variable][v]+bins[target_variable][v+1])/2)\n",
    "            fit_data = fit_data.append(predict_data_day.loc[h-3:h-3]).reset_index(drop = True) \n",
    "        forecast_aux['Date'] = forecast_date\n",
    "        forecast_aux['Hour'] = forecast_hour\n",
    "        forecast_aux['Emissions Forecast'] = smooth(aux_fore,3)\n",
    "        real_value = real_values(pais, i)\n",
    "        forecast_aux[target_variable] = real_value[target_variable]\n",
    "        forecast_values = forecast_values.append(forecast_aux)\n",
    "        tf_inf = time.time()\n",
    "        timeinference.append(tf_inf-ti_inf)\n",
    "    k = k+1\n",
    "#save the results on postgres\n",
    "df_edges = pd.DataFrame()\n",
    "df_edges['edges'] = edges_possibilities\n",
    "df_edges['frequencia'] = edges_frequency\n",
    "df_edges['total days'] = k-1\n",
    "df_time_model = pd.DataFrame()\n",
    "df_time_model['tempo'] = timemodel\n",
    "df_time_inference = pd.DataFrame()\n",
    "df_time_inference['tempo'] = timeinference\n",
    "\n",
    "df_edges.to_sql(name='edges_frequency_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "df_time_model.to_sql(name='time_model_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "df_time_inference.to_sql(name='time_inference_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "forecast_values.to_sql(name='forecast_final2_'+str(pais), con = get_connection(),schema = 'results', if_exists = 'replace', chunksize = None, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50bf3a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emission</th>\n",
       "      <th>Emission-1</th>\n",
       "      <th>Lignite</th>\n",
       "      <th>Lignite-1</th>\n",
       "      <th>Hard coal</th>\n",
       "      <th>Hard coal-1</th>\n",
       "      <th>W. Onshore</th>\n",
       "      <th>Fossil Gas</th>\n",
       "      <th>W. Onshore-1</th>\n",
       "      <th>Fossil Gas-1</th>\n",
       "      <th>Nuclear</th>\n",
       "      <th>Nuclear-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Emission  Emission-1  Lignite  Lignite-1  Hard coal  Hard coal-1  \\\n",
       "0          8           3        7          1          4            7   \n",
       "1          6           3        5          1          3            6   \n",
       "2          6           3        5          1          3            6   \n",
       "3          6           8        5          7          3            4   \n",
       "4          5           6        4          5          3            3   \n",
       "5          5           6        4          5          3            3   \n",
       "6          4           6        4          5          3            3   \n",
       "7          4           5        4          4          3            3   \n",
       "8          4           5        4          4          3            3   \n",
       "9          4           4        4          4          4            3   \n",
       "10         4           4        4          4          4            3   \n",
       "11         4           4        4          4          4            3   \n",
       "12         4           4        4          4          4            4   \n",
       "13         4           4        4          4          4            4   \n",
       "14         4           4        4          4          4            4   \n",
       "15         4           4        4          4          4            4   \n",
       "16         3           4        4          4          4            4   \n",
       "17         3           4        4          4          4            4   \n",
       "18         3           4        4          4          4            4   \n",
       "19         3           3        4          4          4            4   \n",
       "20         3           3        4          4          4            4   \n",
       "21         4           3        4          4          4            4   \n",
       "22         4           3        4          4          4            4   \n",
       "23         4           3        4          4          4            4   \n",
       "\n",
       "    W. Onshore  Fossil Gas  W. Onshore-1  Fossil Gas-1  Nuclear  Nuclear-1  \n",
       "0           17           5            27             4       33         20  \n",
       "1           19           4            27             4       31         20  \n",
       "2           20           3            26             4       27         20  \n",
       "3           21           3            17             5       23         33  \n",
       "4           23           3            19             4       22         31  \n",
       "5           24           3            20             3       22         27  \n",
       "6           26           3            21             3       22         23  \n",
       "7           26           4            23             3       22         22  \n",
       "8           27           4            24             3       22         22  \n",
       "9           27           4            26             3       21         22  \n",
       "10          28           4            26             4       19         22  \n",
       "11          29           4            27             4       20         22  \n",
       "12          30           4            27             4       21         21  \n",
       "13          30           4            28             4       21         19  \n",
       "14          31           4            29             4       21         20  \n",
       "15          31           4            30             4       23         21  \n",
       "16          32           4            30             4       24         21  \n",
       "17          32           4            31             4       25         21  \n",
       "18          32           4            31             4       24         23  \n",
       "19          32           4            32             4       22         24  \n",
       "20          32           4            32             4       19         25  \n",
       "21          32           4            32             4       16         24  \n",
       "22          31           4            32             4       16         22  \n",
       "23          31           4            32             4       15         19  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bdeabc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
